{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        self._grad_fn = None\n",
    "\n",
    "    def set_grad_fn(self, grad_fn):\n",
    "        self._grad_fn = grad_fn\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        if grad is None:\n",
    "            grad = np.ones_like(self.data)\n",
    "\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "        if self._grad_fn is not None:\n",
    "            self._grad_fn.backward(grad)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Add.apply(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Mul.apply(self, other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return Neg.apply(self)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        return Pow.apply(self, power)\n",
    "\n",
    "    def matmul(self, other):\n",
    "        return MatMul.apply(self, other)\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        return Sum.apply(self, axis, keepdims)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, requires_grad={self.requires_grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add:\n",
    "    @staticmethod\n",
    "    def apply(a, b):\n",
    "        c = Tensor(a.data + b.data, requires_grad=a.requires_grad or b.requires_grad)\n",
    "        c.set_grad_fn(Add(a, b))\n",
    "        return c\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.a.requires_grad:\n",
    "            self.a.backward(grad)\n",
    "        if self.b.requires_grad:\n",
    "            self.b.backward(grad)\n",
    "\n",
    "\n",
    "class Mul:\n",
    "    @staticmethod\n",
    "    def apply(a, b):\n",
    "        c = Tensor(a.data * b.data, requires_grad=a.requires_grad or b.requires_grad)\n",
    "        c.set_grad_fn(Mul(a, b))\n",
    "        return c\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.a.requires_grad:\n",
    "            self.a.backward(grad * self.b.data)\n",
    "        if self.b.requires_grad:\n",
    "            self.b.backward(grad * self.a.data)\n",
    "\n",
    "\n",
    "class Neg:\n",
    "    @staticmethod\n",
    "    def apply(a):\n",
    "        c = Tensor(-a.data, requires_grad=a.requires_grad)\n",
    "        c.set_grad_fn(Neg(a))\n",
    "        return c\n",
    "\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.a.requires_grad:\n",
    "            self.a.backward(-grad)\n",
    "\n",
    "\n",
    "class Pow:\n",
    "    @staticmethod\n",
    "    def apply(a, power):\n",
    "        c = Tensor(a.data**power, requires_grad=a.requires_grad)\n",
    "        c.set_grad_fn(Pow(a, power))\n",
    "        return c\n",
    "\n",
    "    def __init__(self, a, power):\n",
    "        self.a = a\n",
    "        self.power = power\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.a.requires_grad:\n",
    "            self.a.backward(grad * self.power * (self.a.data ** (self.power - 1)))\n",
    "\n",
    "\n",
    "class MatMul:\n",
    "    @staticmethod\n",
    "    def apply(a, b):\n",
    "        c = Tensor(\n",
    "            np.dot(a.data, b.data), requires_grad=a.requires_grad or b.requires_grad\n",
    "        )\n",
    "        c.set_grad_fn(MatMul(a, b))\n",
    "        return c\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.a.requires_grad:\n",
    "            self.a.backward(np.dot(grad, self.b.data.T))\n",
    "        if self.b.requires_grad:\n",
    "            self.b.backward(np.dot(self.a.data.T, grad))\n",
    "\n",
    "\n",
    "class Sum:\n",
    "    @staticmethod\n",
    "    def apply(a, axis=None, keepdims=False):\n",
    "        data = a.data.sum(axis=axis, keepdims=keepdims)\n",
    "        c = Tensor(data, requires_grad=a.requires_grad)\n",
    "        c.set_grad_fn(Sum(a, axis, keepdims))\n",
    "        return c\n",
    "\n",
    "    def __init__(self, a, axis, keepdims):\n",
    "        self.a = a\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.a.requires_grad:\n",
    "            shape = np.ones_like(self.a.data.shape)\n",
    "            if self.axis is not None:\n",
    "                shape[self.axis] = self.a.data.shape[self.axis]\n",
    "            self.a.backward(grad.reshape(shape) if not self.keepdims else grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weights = Tensor(np.random.randn(in_features, out_features) * np.sqrt(2. / in_features), requires_grad=True)\n",
    "        self.bias = Tensor(np.zeros(out_features), requires_grad=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.matmul(self.weights) + self.bias\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            param.grad = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 3 * X + 2 + np.random.randn(100, 1) * 0.1  # y = 3x + 2 + noise\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = Tensor(X, requires_grad=False)\n",
    "y_tensor = Tensor(y, requires_grad=False)\n",
    "\n",
    "# Define a simple linear model\n",
    "class SimpleLinearModel:\n",
    "    def __init__(self):\n",
    "        self.linear = Linear(1, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.linear.parameters()\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = SimpleLinearModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X_tensor)\n",
    "    loss = ((predictions - y_tensor) ** 2).sum()  # Mean Squared Error\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.data}\")\n",
    "\n",
    "# Print final parameters\n",
    "print(f\"Weights: {model.linear.weights.data.flatten()}\")\n",
    "print(f\"Bias: {model.linear.bias.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
