{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_odyssey.txt                                    has 698,079 words\n",
      "moby_dick.txt                                      has 1,238,224 words\n",
      "alice_wonderland.txt                               has 163,916 words\n",
      "frankenstein.txt                                   has 438,808 words\n",
      "dracula.txt                                        has 865,171 words\n",
      "a_tale_of_two_cities.txt                           has 776,878 words\n",
      "pride_and_prejudice.txt                            has 748,121 words\n",
      "the_complete_works_of_william_shakespeare.txt      has 5,378,662 words\n",
      "a_room_with_a_view.txt                             has 394,369 words\n",
      "metamorphosis.txt                                  has 138,257 words\n",
      "the_great_gatspy.txt                               has 290,075 words\n",
      "adventures_of_huckleberry_finn.txt                 has 590,377 words\n",
      "the_iliad.txt                                      has 1,116,791 words\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"data/tinystories/\"\n",
    "corpus = \"\"\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    with open(path + filename, \"r\") as f:\n",
    "        file = f.read()\n",
    "        print(f\"{filename:50} has {len(file):,} words\")\n",
    "        corpus += file + \"\\n\"\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"projects/4-stories/tokenizer.json\")\n",
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"alice_wonderland.txt\", \"r\") as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded corpus has 52,316 tokens\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(corpus).ids\n",
    "print(f\"Encoded corpus has {len(encoded):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 41,852 tokens\n",
      "Validation data has 10,464 tokens\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "train_size = int(len(encoded) * train_split)\n",
    "train_data = encoded[:train_size]\n",
    "val_data = encoded[train_size:]\n",
    "\n",
    "print(f\"Training data has {len(train_data):,} tokens\")\n",
    "print(f\"Validation data has {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset and round to nearest 256\n",
    "train_dataset = torch.tensor(train_data[:-(len(train_data) % 128)]).view(-1, 128)\n",
    "val_dataset = torch.tensor(val_data[:-(len(val_data) % 128)]).view(-1, 128)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.GPT2Config(\n",
    "    vocab_size=4096,\n",
    "    n_positions=512,\n",
    "    n_embd=128,\n",
    "    n_layer=8,\n",
    "    n_head=4,\n",
    "    n_inner=None, # default 4 * n_embd\n",
    ")\n",
    "model = transformers.GPT2LMHeadModel(config).to(device)\n",
    "num_train_steps = 0\n",
    "print(f\"Model using {model.num_parameters():,} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm.tqdm(loader, desc=\"Evaluation\"):\n",
    "        batch = batch.to(device)\n",
    "        x, y = batch[:, :-1], batch[:, 1:]\n",
    "        \n",
    "        output = model(x, labels=y)\n",
    "        \n",
    "        loss = output.loss\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_pytorch import EMA\n",
    "ema = EMA(model, beta=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "pbar = tqdm.tqdm(range(20), desc=\"Training\")\n",
    "for epoch in pbar:\n",
    "    # test_loss = evaluate(model, train_loader)\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(train_loader, leave=True, desc=f\"Epoch {epoch}\")\n",
    "    for seq in pbar:\n",
    "        seq = seq.to(device)\n",
    "        x = seq[:, :-1]\n",
    "        y = seq[:, 1:]\n",
    "\n",
    "        optim.zero_grad()\n",
    "        output = model(x, labels=y)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        ema.update()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        pbar.set_postfix_str(f\"Loss: {loss.item():.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new name\n",
    "context = torch.tensor(tokenizer.encode(\"Once upon a time, there was a young prince named\").ids, device=device).unsqueeze(0).long()\n",
    "generated = model.generate(context, max_length=100)\n",
    "print(tokenizer.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(model.state_dict(), 'projects/4-shakespeare/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
