{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def noiser(img, noise, step, steps):\n",
    "    \"\"\"returns a noisy image\"\"\"\n",
    "\n",
    "    noise_scale = step / (steps - 1)\n",
    "    return img * (noise_scale) + noise * (1 - noise_scale)\n",
    "\n",
    "\n",
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "\n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** (\n",
    "        (\n",
    "            torch.arange(\n",
    "                start=0,\n",
    "                end=temb_dim // 2,\n",
    "                dtype=torch.float32,\n",
    "                device=time_steps.device,\n",
    "            )\n",
    "            / (temb_dim // 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample using 2x2 average pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        t_emb_dim,\n",
    "        down_sample=True,\n",
    "        num_heads=4,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels if i == 0 else out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.SiLU(), nn.Linear(t_emb_dim, out_channels))\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(4, out_channels) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels if i == 0 else out_channels, out_channels, kernel_size=1\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = (\n",
    "            nn.Conv2d(out_channels, out_channels, 4, 2, 1)\n",
    "            if self.down_sample\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "            # Attention block of Unet\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels if i == 0 else out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.SiLU(), nn.Linear(t_emb_dim, out_channels))\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(4, out_channels) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels if i == 0 else out_channels, out_channels, kernel_size=1\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "\n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        t_emb_dim,\n",
    "        up_sample=True,\n",
    "        num_heads=4,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels if i == 0 else out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.SiLU(), nn.Linear(t_emb_dim, out_channels))\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(4, out_channels) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels if i == 0 else out_channels, out_channels, kernel_size=1\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = (\n",
    "            nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 4, 2, 1)\n",
    "            if self.up_sample\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Unet model comprising\n",
    "    Down blocks, Midblocks and Uplocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        im_channels = model_config[\"im_channels\"]\n",
    "        self.down_channels = model_config[\"down_channels\"]\n",
    "        self.mid_channels = model_config[\"mid_channels\"]\n",
    "        self.t_emb_dim = model_config[\"time_emb_dim\"]\n",
    "        self.down_sample = model_config[\"down_sample\"]\n",
    "        self.num_down_layers = model_config[\"num_down_layers\"]\n",
    "        self.num_mid_layers = model_config[\"num_mid_layers\"]\n",
    "        self.num_up_layers = model_config[\"num_up_layers\"]\n",
    "\n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "\n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "        )\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1)\n",
    "        )\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(\n",
    "                DownBlock(\n",
    "                    self.down_channels[i],\n",
    "                    self.down_channels[i + 1],\n",
    "                    self.t_emb_dim,\n",
    "                    down_sample=self.down_sample[i],\n",
    "                    num_layers=self.num_down_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(\n",
    "                MidBlock(\n",
    "                    self.mid_channels[i],\n",
    "                    self.mid_channels[i + 1],\n",
    "                    self.t_emb_dim,\n",
    "                    num_layers=self.num_mid_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels) - 1)):\n",
    "            self.ups.append(\n",
    "                UpBlock(\n",
    "                    self.down_channels[i] * 2,\n",
    "                    self.down_channels[i - 1] if i != 0 else 16,\n",
    "                    self.t_emb_dim,\n",
    "                    up_sample=self.down_sample[i],\n",
    "                    num_layers=self.num_up_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(4, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        out = self.conv_in(x)\n",
    "        # B x C1 x H x W\n",
    "\n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "\n",
    "        down_outs = []\n",
    "\n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "\n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "\n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out\n",
    "\n",
    "\n",
    "class LitDDPM(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, down_features, middle_features, num_layers, embed_dim, diffusion_steps, optimizer, model\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        for t in range(self.diffusion_steps - 1):\n",
    "            x = self.model(x, torch.tensor([t] * x.shape[0], device=x.device))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        noise = torch.randn_like(x, device=x.device)\n",
    "        total_loss = 0\n",
    "        optim = self.optimizers()\n",
    "\n",
    "        for i in range(self.diffusion_steps - 1):\n",
    "            optim.zero_grad()\n",
    "            noisy_img = noiser(x, noise, i, self.diffusion_steps)\n",
    "            target_img = noiser(x, noise, i + 1, self.diffusion_steps)\n",
    "\n",
    "            generated_img = self.model(\n",
    "                noisy_img, torch.tensor([i] * noisy_img.shape[0], device=noisy_img.device)\n",
    "            )\n",
    "\n",
    "            loss = self.loss(generated_img, target_img)\n",
    "            self.manual_backward(loss)\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        self.log(\"train_loss\", total_loss, on_step=True, prog_bar=True)\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            noise = torch.randn_like(x, device=x.device)\n",
    "            generated = self.forward(x)\n",
    "            grid = torchvision.utils.make_grid(generated)\n",
    "\n",
    "            self.logger.experiment.log(\n",
    "                {\n",
    "                    \"generated\": wandb.Image(\n",
    "                        grid, caption=f\"Epoch {self.current_epoch}, Batch {batch_idx}\"\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=\"data/celeba_hq\", transform=torchvision.transforms.Compose([torchvision.transforms.Resize((64, 64)), torchvision.transforms.ToTensor()])\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    {\n",
    "        \"im_channels\": 3,\n",
    "        \"down_channels\": [4, 8, 16, 32],\n",
    "        \"mid_channels\": [32, 32, 16],\n",
    "        \"time_emb_dim\": 64,\n",
    "        \"down_sample\": [True, True, False],\n",
    "        \"num_down_layers\": 1,\n",
    "        \"num_mid_layers\": 1,\n",
    "        \"num_up_layers\": 1,\n",
    "    }    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "lit_model = LitDDPM(\n",
    "    down_features=4,\n",
    "    middle_features=32,\n",
    "    num_layers=2,\n",
    "    embed_dim=64,\n",
    "    diffusion_steps=100,\n",
    "    optimizer=optimizer,\n",
    "    model=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(project=\"ddpm\")\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | Unet    | 118 K \n",
      "1 | loss  | MSELoss | 0     \n",
      "----------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.474     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e644003559944ee6b49e48036f81fcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
