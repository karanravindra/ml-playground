{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.QMNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.QMNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=60, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=60, shuffle=False)\n",
    "\n",
    "print(f\"Training with {len(train_dataset)} samples\")\n",
    "print(f\"Testing with {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 4\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH\n",
    "IMAGE_CHANNELS = 1\n",
    "EMBEDDING_DIMS = IMAGE_CHANNELS * PATCH_SIZE**2\n",
    "NUM_OF_PATCHES = int((IMAGE_WIDTH * IMAGE_HEIGHT) / PATCH_SIZE**2)\n",
    "\n",
    "assert IMAGE_WIDTH % PATCH_SIZE == 0 and IMAGE_HEIGHT % PATCH_SIZE ==0 , print(\"Image Width is not divisible by patch size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=2)\n",
    "        self.class_token_embeddings = nn.Parameter(torch.rand((60, 1, EMBEDDING_DIMS), requires_grad=True))\n",
    "        self.position_embeddings = nn.Parameter(torch.rand((1, NUM_OF_PATCHES + 1, EMBEDDING_DIMS), requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat((self.class_token_embeddings, self.flatten_layer(self.conv_layer(x).permute((0, 2, 3, 1)))), dim=1) + self.position_embeddings\n",
    "        return output\n",
    "    \n",
    "patch_embedding_layer = PatchEmbeddingLayer(in_channels=IMAGE_CHANNELS, patch_size=PATCH_SIZE, embedding_dim=IMAGE_CHANNELS * PATCH_SIZE ** 2)\n",
    "\n",
    "patch_embeddings = patch_embedding_layer(next(iter(train_loader))[0])\n",
    "patch_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dims=768,  # Hidden Size D in the ViT Paper Table 1\n",
    "        num_heads=12,  # Heads in the ViT Paper Table 1\n",
    "        attn_dropout=0.0,  # Default to Zero as there is no dropout for the the MSA Block as per the ViT Paper\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.num_head = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape=embedding_dims)\n",
    "\n",
    "        self.multiheadattention = nn.MultiheadAttention(\n",
    "            num_heads=num_heads,\n",
    "            embed_dim=embedding_dims,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm(x)\n",
    "        output, _ = self.multiheadattention(query=x, key=x, value=x, need_weights=False)\n",
    "        return output\n",
    "\n",
    "\n",
    "multihead_self_attention_block = MultiHeadSelfAttentionBlock(\n",
    "    embedding_dims=EMBEDDING_DIMS, num_heads=2\n",
    ")\n",
    "print(\n",
    "    f\"Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]\"\n",
    ")\n",
    "print(\n",
    "    f\"Shape of the output from MSA Block => {list(multihead_self_attention_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningPerceptronBlock(nn.Module):\n",
    "  def __init__(self, embedding_dims, mlp_size, mlp_dropout):\n",
    "    super().__init__()\n",
    "    self.embedding_dims = embedding_dims\n",
    "    self.mlp_size = mlp_size\n",
    "    self.dropout = mlp_dropout\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(normalized_shape = embedding_dims)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features = embedding_dims, out_features = mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p = mlp_dropout),\n",
    "        nn.Linear(in_features = mlp_size, out_features = embedding_dims),\n",
    "        nn.Dropout(p = mlp_dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.mlp(self.layernorm(x))\n",
    "\n",
    "mlp_block = MachineLearningPerceptronBlock(embedding_dims = EMBEDDING_DIMS,\n",
    "                                           mlp_size = 3072,\n",
    "                                           mlp_dropout = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dims=768,\n",
    "        mlp_dropout=0.1,\n",
    "        attn_dropout=0.0,\n",
    "        mlp_size=3072,\n",
    "        num_heads=12,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.msa_block = MultiHeadSelfAttentionBlock(\n",
    "            embedding_dims=embedding_dims,\n",
    "            num_heads=num_heads,\n",
    "            attn_dropout=attn_dropout,\n",
    "        )\n",
    "\n",
    "        self.mlp_block = MachineLearningPerceptronBlock(\n",
    "            embedding_dims=embedding_dims,\n",
    "            mlp_size=mlp_size,\n",
    "            mlp_dropout=mlp_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(x) + x\n",
    "        x = self.mlp_block(x) + x\n",
    "\n",
    "        return x\n",
    "    \n",
    "transformer_block = TransformerBlock(embedding_dims = EMBEDDING_DIMS,\n",
    "                                     mlp_dropout = 0.1,\n",
    "                                     attn_dropout=0.0,\n",
    "                                     mlp_size = 3072,\n",
    "                                     num_heads = 2)\n",
    "\n",
    "print(f'Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n",
    "print(f'Shape of the output from Transformer Block => {list(transformer_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=28,\n",
    "        in_channels=1,\n",
    "        patch_size=4,\n",
    "        embedding_dims=16,\n",
    "        num_transformer_layers=12,  # from table 1 above\n",
    "        mlp_dropout=0.1,\n",
    "        attn_dropout=0.0,\n",
    "        mlp_size=64,\n",
    "        num_heads=2,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding_layer = PatchEmbeddingLayer(\n",
    "            in_channels=in_channels, patch_size=patch_size, embedding_dim=embedding_dims\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embedding_dims=embedding_dims,\n",
    "                    mlp_dropout=mlp_dropout,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                    mlp_size=mlp_size,\n",
    "                    num_heads=num_heads,\n",
    "                )\n",
    "                for _ in range(num_transformer_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dims),\n",
    "            nn.Linear(in_features=embedding_dims, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(\n",
    "            self.transformer_encoder(self.patch_embedding_layer(x))[:, 0]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT()\n",
    "criterion = F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    with tqdm.tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for data, target in tepoch:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            if data.shape[0] != 64:\n",
    "                print(data.shape)\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix_str(f\"Loss: {loss.item()}\")\n",
    "            \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "        print(f\"Accuracy: {correct / total}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    with tqdm.tqdm(test_loader, unit=\"batch\") as pbar:\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            pbar.set_postfix_str(f\"Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
