{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Review: Cifar10 with PyTorch\n",
    "![image](output.png)\n",
    "## Introduction\n",
    "This notebook is a review of the architecture used in the PyTorch tutorial for the CIFAR10 dataset. This is the complete code for the tutorial, with some minor modifications to make it more readable and to modify the architectures for this dataset. To find the practice notebook, please visit the following link: [Practice Notebook](). \n",
    "\n",
    "### Pre-requisites\n",
    "This notebook assumes that you have a basic understanding of neural networks and PyTorch. If you are new to PyTorch, please visit the following link to get started: [PyTorch Tutorials](https://pytorch.org/tutorials/). Please complete the [MNIST tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) before starting this tutorial.\n",
    "\n",
    "### Dataset\n",
    "The CIFAR10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, and other similar vehicles. \"Truck\" includes only big trucks. The test batch contains 1,000 randomly selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5,000 images from each class. While you may realize that this doesn't sound like it's useful for real-world applications, it's a good dataset to start with and learn the basics of deep learning since it's small and easy to work with. \n",
    "\n",
    "**Classes**:\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck\n",
    "- [CIFAR10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [CIFAR10 Dataset PyTorch](https://pytorch.org/vision/stable/datasets.html#cifar)\n",
    "\n",
    "**Critcism**:\n",
    "The CIFAR10 dataset is a very small dataset and is not representative of real-world data. It is used for educational purposes and to learn the basics of deep learning. Intersting read: [Once Upon a Time in CIFAR-10](https://franky07724-57962.medium.com/once-upon-a-time-in-cifar-10-c26bb056b4ce#:~:text=However%2C%20the%20quality%20of%20CIFAR,10%20contains%200.54%25%20label%20errors.)\n",
    "\n",
    "\n",
    "**Note**: \n",
    "There is also a CIFAR100 dataset, which has 100 classes. For the extra challenge, you can try working with the CIFAR100 dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code setup\n",
    "Let's start by importing the necessary libraries and setting up the code for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also set the seed for the random number generator to ensure reproducibility. This is important when working with neural networks since the weights are initialized randomly, and the order of the data can affect the training process. By setting the seed, we ensure that the random number generator produces the same sequence of numbers every time we run the code.\n",
    "\n",
    "We should also set the device to use the 'cuda' (NVIDIA GPU), 'mps' (Mac GPU), or 'cpu', depending on the availability of the GPU. If you are using Google Colab, you can change the runtime to use a GPU by going to 'Runtime' -> 'Change runtime type' -> 'Hardware accelerator' -> 'GPU'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.backends.cudnn.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to load the CIFAR10 dataset using the `torchvision` library. The dataset is divided into training and testing sets, and each set contains images and their corresponding labels (0-9). The images are also normalized to have a mean of 0.5 and a standard deviation of 0.5. This normalization helps the neural network since it allows the weights to be updated more evenly during training.\n",
    "\n",
    "The batch size is set to 64, though you can change it to a different value if you like. The batch size determines how many images are processed at once during training. A larger batch size can speed up training but requires more memory. A smaller batch size can slow down training but may lead to better generalization.\n",
    "\n",
    "Finally, we create a DataLoader for the training and testing sets. The DataLoader is used to load the data in batches during training and testing. It also shuffles the data during training to ensure that the model learns from different samples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # mean, std\n",
    "])\n",
    "\n",
    "train_dataset = tv.datasets.CIFAR10(\"./data\", train=True, transform=transforms, download=True)\n",
    "test_dataset = tv.datasets.CIFAR10(\"./data\", train=False, transform=transforms, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also deifine an `evaluate_model` function to evaluate the model on a given dataset. This function calculates the accuracy of the model on the dataset and returns the accuracy.\n",
    "\n",
    "Then we define a `train_model` function to train the model on the training dataset. This function takes the model, optimizer, loss function, number of epochs, and the training DataLoader as input. It trains the model for the specified number of epochs and returns the trained model.\n",
    "\n",
    "And a `plot` function to plot the training and training and testing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, loader: torch.utils.data.DataLoader, criterion: nn.Module) -> Tuple[float, float]:\n",
    "    \"\"\" Evaluate the model on the given dataset and return both average accuracy and average loss. \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm.tqdm(\n",
    "            loader, leave=False, desc=\"Evaluating\", total=len(loader)\n",
    "        ):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss += loss.item() * x.size(0)  # Accumulate loss scaled by batch size\n",
    "            correct += (y_pred.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    average_loss = total_loss / total  # Calculate the average loss\n",
    "    accuracy = correct / total\n",
    "    return accuracy, average_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    momentum: float,\n",
    ") -> Tuple[Tuple[List[float], List[float]], Tuple[List[float], List[float]]]:\n",
    "    \"\"\" Train the model on the given dataset and track accuracies and losses. \"\"\"\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    with tqdm.tqdm(range(epochs), desc=\"Training\", unit=\"epoch\") as epochs_bar:\n",
    "        for epoch in epochs_bar:\n",
    "            model.train()\n",
    "            train_epoch_losses = []\n",
    "            for x, y in tqdm.tqdm(train_loader, leave=False, desc=\"Epoch\", total=len(train_loader)):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                train_epoch_losses.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Compute average loss for the epoch\n",
    "            avg_train_loss = sum(train_epoch_losses) / len(train_epoch_losses)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Evaluate model for both training and testing sets\n",
    "            train_accuracy, train_loss = evaluate_model(model, train_loader, criterion)\n",
    "            test_accuracy, test_loss = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            epochs_bar.set_postfix(\n",
    "                train_loss=train_loss,\n",
    "                test_loss=test_loss,\n",
    "                train_accuracy=train_accuracy,\n",
    "                test_accuracy=test_accuracy\n",
    "            )\n",
    "\n",
    "    return (train_accuracies, test_accuracies), (train_losses, test_losses)\n",
    "\n",
    "\n",
    "def plot(accuracies: Tuple[List[float], List[float]], losses: Tuple[List[float], List[float]]) -> None:\n",
    "    \"\"\"Plot the training progress of the model.\"\"\"\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(accuracies[0], label=\"Training Accuracy\", color='b', linewidth=2)\n",
    "    plt.plot(accuracies[1], label=\"Test Accuracy\", color='r', linewidth=2)\n",
    "    \n",
    "    # Compute and plot EMA for training accuracies\n",
    "    ema_train = [accuracies[0][0]]  # Start EMA from the first actual accuracy\n",
    "    alpha = 0.2  # Smoothing factor for EMA\n",
    "    for loss in accuracies[0][1:]:\n",
    "        ema = alpha * loss + (1 - alpha) * ema_train[-1]\n",
    "        ema_train.append(ema)\n",
    "    plt.plot(ema_train, label=\"EMA Training Accuracy\", linestyle=\"--\", color='darkblue', alpha=0.75)\n",
    "    \n",
    "    # Compute and plot EMA for test accuracies\n",
    "    ema_test = [accuracies[1][0]]  # Start EMA from the first actual accuracy\n",
    "    for loss in accuracies[1][1:]:\n",
    "        ema = alpha * loss + (1 - alpha) * ema_test[-1]\n",
    "        ema_test.append(ema)\n",
    "    plt.plot(ema_test, label=\"EMA Test Accuracy\", linestyle=\"--\", color='darkred', alpha=0.75)\n",
    "    \n",
    "    plt.title(\"Accuracy Progress\", fontsize=16)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses[0], label=\"Training Loss\", color='b', linewidth=2)\n",
    "    plt.plot(losses[1], label=\"Test Loss\", color='r', linewidth=2)\n",
    "    \n",
    "    # Compute and plot EMA for training accuracies\n",
    "    ema_train = [losses[0][0]]  # Start EMA from the first actual accuracy\n",
    "    alpha = 0.5  # Smoothing factor for EMA\n",
    "    for loss in losses[0][1:]:\n",
    "        ema = alpha * loss + (1 - alpha) * ema_train[-1]\n",
    "        ema_train.append(ema)\n",
    "    plt.plot(ema_train, label=\"EMA Training Loss\", linestyle=\"--\", color='darkblue', alpha=0.75)\n",
    "    \n",
    "    # Compute and plot EMA for test accuracies\n",
    "    ema_test = [losses[1][0]]  # Start EMA from the first actual accuracy\n",
    "    for loss in losses[1][1:]:\n",
    "        ema = alpha * loss + (1 - alpha) * ema_test[-1]\n",
    "        ema_test.append(ema)\n",
    "    plt.plot(ema_test, label=\"EMA Test Loss\", linestyle=\"--\", color='darkred', alpha=0.75)\n",
    "    \n",
    "    plt.title(\"Loss Progress\", fontsize=16)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define a simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32*32*3, 10)\n",
    ").to(device)\n",
    "\n",
    "# Train the model \n",
    "metrics = train_model(model, train_loader, test_loader, epochs=10, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Plot the training progress\n",
    "plot(*metrics)\n",
    "\n",
    "# Print final accuracies\n",
    "print(f\"Final training accuracies: {metrics[0][0][-5:]}\")\n",
    "print(f\"Final test accuracies: {metrics[0][1][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define a simple model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(32*32*3, 512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10)\n",
    ").to(device)\n",
    "\n",
    "# Train the model \n",
    "metrics = train_model(model, train_loader, test_loader, epochs=10, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Plot the training progress\n",
    "plot(*metrics)\n",
    "\n",
    "# Print final accuracies\n",
    "print(f\"Final training accuracies: {metrics[0][0][-5:]}\")\n",
    "print(f\"Final test accuracies: {metrics[0][1][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 6, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),\n",
    ").to(device)\n",
    "\n",
    "# Train the model \n",
    "metrics = train_model(model, train_loader, test_loader, epochs=10, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Plot the training progress\n",
    "plot(*metrics)\n",
    "\n",
    "# Print final accuracies\n",
    "print(f\"Final training accuracies: {metrics[0][0][-5:]}\")\n",
    "print(f\"Final test accuracies: {metrics[0][1][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Modern CNNs and better Training Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64*5*5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ").to(device)\n",
    "\n",
    "metrics = train_model(model, train_loader, test_loader, epochs=100, lr=0.01, momentum=0.9)\n",
    "print(f\"Final training accuracy: {train_metrics[-5:]}\")\n",
    "print(f\"Final test accuracy: {test_metrics[-5:]}\")\n",
    "\n",
    "plot(*metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.BatchNorm2d(3),\n",
    "    nn.Conv2d(3, 32, kernel_size=3, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.BatchNorm1d(64*6*6),\n",
    "    nn.Linear(64*6*6, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(120),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(84),\n",
    "    nn.Linear(84, 10)\n",
    ").to(device)\n",
    "\n",
    "# Train the model \n",
    "metrics = train_model(model, train_loader, test_loader, epochs=10, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Plot the training progress\n",
    "plot(*metrics)\n",
    "\n",
    "# Print final accuracies\n",
    "print(f\"Final training accuracies: {metrics[0][0][-5:]}\")\n",
    "print(f\"Final test accuracies: {metrics[0][1][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Train the model \n",
    "metrics = train_model(model, train_loader, test_loader, epochs=10, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Plot the training progress\n",
    "plot(*metrics)\n",
    "\n",
    "# Print final accuracies\n",
    "print(f\"Final training accuracies: {metrics[0][0][-5:]}\")\n",
    "print(f\"Final test accuracies: {metrics[0][1][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Train the model \n",
    "metrics = train_model(model, train_loader, test_loader, epochs=10, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Plot the training progress\n",
    "plot(*metrics)\n",
    "\n",
    "# Print final accuracies\n",
    "print(f\"Final training accuracies: {metrics[0][0][-5:]}\")\n",
    "print(f\"Final test accuracies: {metrics[0][1][-5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
