{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f946ec0b460>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using 10,936,577 parameters.\n",
      "Model using 10,936,577 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tqdm.notebook as tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "n_embd = 384 # how many embedding dimensions?\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.1\n",
    "vocab_size = 257\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# ------------\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.reshape(B*T)\n",
    "                        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in tqdm.trange(max_new_tokens, leave=False):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "generator = GPTLanguageModel().to(device)\n",
    "# print the number of parameters in the model\n",
    "print(f\"Model using {sum(p.numel() for p in generator.parameters()):,} parameters.\")\n",
    "print(f\"Model using {sum(p.numel() for p in generator.parameters() if p.requires_grad):,} trainable parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .pt file\n",
    "quantized_dataset = torch.load(\"quantized.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_loader = torch.utils.data.DataLoader(quantized_dataset, batch_size=64, pin_memory=True, shuffle=True)\n",
    "x = next(iter(quantized_loader))\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(generator.parameters(), lr=9e-4, weight_decay=0.1, betas=(0.9, 0.95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema = EMA(generator, beta=0.999, update_after_step=1, update_every=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.train()\n",
    "for epoch in range(10):\n",
    "    with tqdm.tqdm(quantized_loader, desc=\"Batch\") as pbar:\n",
    "        for x in pbar:\n",
    "            x = x.to(device) + 1\n",
    "            \n",
    "            if x.min() < 1 or x.max() >= 257:\n",
    "                print(x.min(), x.max())\n",
    "                continue\n",
    "            \n",
    "            for i in tqdm.tnrange(1, x.shape[1]-1, desc=\"Sequence\"):\n",
    "                inputs = x[:, :i]\n",
    "                targets = x[:, i+1]\n",
    "                \n",
    "                inputs = F.pad(inputs, (block_size - inputs.shape[1], 0), value=0)\n",
    "                \n",
    "                logits = generator(inputs)[0][:, -1, :]\n",
    "                \n",
    "                loss = criterion(logits, targets)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")\n",
    "                \n",
    "            ema.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# generate from the model\n",
    "with torch.no_grad():\n",
    "    generator.eval()\n",
    "    model.eval()\n",
    "    context = torch.zeros(1, 1, dtype=torch.long, device=device)\n",
    "    idxs = generator.generate(context, max_new_tokens=255)[0] - 1\n",
    "    \n",
    "    \n",
    "    quant = F.embedding(\n",
    "            idxs.view(1, 16, 16),\n",
    "            model.model.vq.e_i_ts.transpose(0, 1)\n",
    "    ).permute(0, 3, 1, 2)\n",
    "        \n",
    "    img = model.model.decoder(quant)\n",
    "        \n",
    "    plt.imshow(img[0].permute(1, 2, 0).detach().cpu().clamp(0, 1))\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.train()\n",
    "with tqdm.tnrange(10, desc=\"Epochs\", leave=True) as epochs:\n",
    "    with tqdm.tqdm(quantized_loader, leave=True, desc=\"Batches\") as progress:\n",
    "        with tqdm.tnrange(0, 256-1, block_size, leave=True, desc=\"Seq\") as seq:\n",
    "            for epoch in epochs:\n",
    "                for x_batch in progress:\n",
    "                    x_batch = x_batch[0]  # Assuming DataLoader returns a tuple\n",
    "                    \n",
    "                    x_batch = x_batch.to(device)\n",
    "                    \n",
    "                    total_loss = torch.tensor(0.0, device=device)\n",
    "                    \n",
    "                    for i in seq:\n",
    "                        x = x_batch[:, i:i+block_size]\n",
    "                        y = x_batch[:, i+1:i+block_size+1]\n",
    "                        \n",
    "                        if y.size(1) < block_size:\n",
    "                            y = F.pad(y, (0, block_size - y.size(1)))\n",
    "                        \n",
    "                        logits, loss = generator(x, y)\n",
    "                        \n",
    "                        total_loss += loss\n",
    "                        \n",
    "                        epochs.set_postfix(loss=loss.item(), total_loss=total_loss.item())\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    seq.close()\n",
    "                            \n",
    "                    ema.update()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"projects/paper-implementations/image-generation/6-vq-vae/generator.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate from the model\n",
    "with torch.no_grad():\n",
    "    # tensor dataset\n",
    "    model.eval()\n",
    "    dataset = torch.load(\"quantized.pt\")\n",
    "    dataset = torch.utils.data.TensorDataset(dataset)\n",
    "    \n",
    "    for idxs in dataset:\n",
    "        \n",
    "        quant = F.embedding(\n",
    "            idxs[0].view(1, 16, 16),\n",
    "            model.model.vq.e_i_ts.transpose(0, 1)\n",
    "        ).permute(0, 3, 1, 2)\n",
    "        \n",
    "        img = model.model.decoder(quant)\n",
    "        \n",
    "        plt.imshow(img[0].permute(1, 2, 0).detach().cpu().clamp(0, 1))\n",
    "        \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        layers = []\n",
    "        for i in range(num_residual_layers):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=num_hiddens,\n",
    "                        out_channels=num_residual_hiddens,\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=num_residual_hiddens,\n",
    "                        out_channels=num_hiddens,\n",
    "                        kernel_size=1,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = h + layer(h)\n",
    "\n",
    "        # ResNet V1-style.\n",
    "        return torch.relu(h)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        num_hiddens,\n",
    "        num_downsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
    "        # off with a ReLU.\n",
    "        conv = nn.Sequential()\n",
    "        for downsampling_layer in range(num_downsampling_layers):\n",
    "            if downsampling_layer == 0:\n",
    "                out_channels = num_hiddens // 2\n",
    "            elif downsampling_layer == 1:\n",
    "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
    "\n",
    "            else:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
    "\n",
    "            conv.add_module(\n",
    "                f\"down{downsampling_layer}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            )\n",
    "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
    "\n",
    "        conv.add_module(\n",
    "            \"final_conv\",\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_hiddens,\n",
    "                out_channels=num_hiddens,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "        self.conv = conv\n",
    "        self.residual_stack = ResidualStack(\n",
    "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        return self.residual_stack(h)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_hiddens,\n",
    "        num_upsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=num_hiddens,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.residual_stack = ResidualStack(\n",
    "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
    "        )\n",
    "        upconv = nn.Sequential()\n",
    "        for upsampling_layer in range(num_upsampling_layers):\n",
    "            if upsampling_layer < num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
    "\n",
    "            elif upsampling_layer == num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
    "\n",
    "            else:\n",
    "                (in_channels, out_channels) = (num_hiddens // 2, 3)\n",
    "\n",
    "            upconv.add_module(\n",
    "                f\"up{upsampling_layer}\",\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            )\n",
    "            if upsampling_layer < num_upsampling_layers - 1:\n",
    "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
    "\n",
    "        self.upconv = upconv\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = self.residual_stack(h)\n",
    "        x_recon = self.upconv(h)\n",
    "        return x_recon\n",
    "\n",
    "\n",
    "class SonnetExponentialMovingAverage(nn.Module):\n",
    "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
    "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
    "    # of \"Neural Discrete Representation Learning\".\n",
    "    def __init__(self, decay, shape):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        self.counter = 0\n",
    "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
    "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
    "\n",
    "    def update(self, value):\n",
    "        self.counter += 1\n",
    "        with torch.no_grad():\n",
    "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
    "            self.average = self.hidden / (1 - self.decay**self.counter)\n",
    "\n",
    "    def __call__(self, value):\n",
    "        self.update(value)\n",
    "        return self.average\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
    "        super().__init__()\n",
    "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
    "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.use_ema = use_ema\n",
    "        # Weight for the exponential moving average.\n",
    "        self.decay = decay\n",
    "        # Small constant to avoid numerical instability in embedding updates.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Dictionary embeddings.\n",
    "        limit = 3**0.5\n",
    "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_(\n",
    "            -limit, limit\n",
    "        )\n",
    "        if use_ema:\n",
    "            self.register_buffer(\"e_i_ts\", e_i_ts)\n",
    "        else:\n",
    "            self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
    "\n",
    "        # Exponential moving average of the cluster counts.\n",
    "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
    "        # Exponential moving average of the embeddings.\n",
    "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
    "        distances = (\n",
    "            (flat_x**2).sum(1, keepdim=True)\n",
    "            - 2 * flat_x @ self.e_i_ts\n",
    "            + (self.e_i_ts**2).sum(0, keepdim=True)\n",
    "        )\n",
    "        encoding_indices = distances.argmin(1)\n",
    "        quantized_x = F.embedding(\n",
    "            encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)\n",
    "        ).permute(0, 3, 1, 2)\n",
    "\n",
    "        # See second term of Equation (3).\n",
    "        if not self.use_ema:\n",
    "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
    "        else:\n",
    "            dictionary_loss = None\n",
    "\n",
    "        # See third term of Equation (3).\n",
    "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
    "        # Straight-through gradient. See Section 3.2.\n",
    "        quantized_x = x + (quantized_x - x).detach()\n",
    "\n",
    "        if self.use_ema and self.training:\n",
    "            with torch.no_grad():\n",
    "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
    "\n",
    "                # Cluster counts.\n",
    "                encoding_one_hots = F.one_hot(\n",
    "                    encoding_indices, self.num_embeddings\n",
    "                ).type(flat_x.dtype)\n",
    "                n_i_ts = encoding_one_hots.sum(0)\n",
    "                # Updated exponential moving average of the cluster counts.\n",
    "                # See Equation (6).\n",
    "                self.N_i_ts(n_i_ts)\n",
    "\n",
    "                # Exponential moving average of the embeddings. See Equation (7).\n",
    "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
    "                self.m_i_ts(embed_sums)\n",
    "\n",
    "                # This is kind of weird.\n",
    "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
    "                # and Equation (8).\n",
    "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
    "                N_i_ts_stable = (\n",
    "                    (self.N_i_ts.average + self.epsilon)\n",
    "                    / (N_i_ts_sum + self.num_embeddings * self.epsilon)\n",
    "                    * N_i_ts_sum\n",
    "                )\n",
    "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
    "\n",
    "        return (\n",
    "            quantized_x,\n",
    "            dictionary_loss,\n",
    "            commitment_loss,\n",
    "            encoding_indices.view(x.shape[0], -1),\n",
    "        )\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        num_hiddens,\n",
    "        num_downsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "        embedding_dim,\n",
    "        num_embeddings,\n",
    "        use_ema,\n",
    "        decay,\n",
    "        epsilon,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            in_channels,\n",
    "            num_hiddens,\n",
    "            num_downsampling_layers,\n",
    "            num_residual_layers,\n",
    "            num_residual_hiddens,\n",
    "        )\n",
    "        self.pre_vq_conv = nn.Conv2d(\n",
    "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
    "        )\n",
    "        self.vq = VectorQuantizer(\n",
    "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim,\n",
    "            num_hiddens,\n",
    "            num_downsampling_layers,\n",
    "            num_residual_layers,\n",
    "            num_residual_hiddens,\n",
    "        )\n",
    "\n",
    "    def quantize(self, x):\n",
    "        z = self.pre_vq_conv(self.encoder(x))\n",
    "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
    "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
    "        x_recon = self.decoder(z_quantized)\n",
    "        return {\n",
    "            \"dictionary_loss\": dictionary_loss,\n",
    "            \"commitment_loss\": commitment_loss,\n",
    "            \"x_recon\": x_recon,\n",
    "        }\n",
    "\n",
    "\n",
    "class VQVAE_Trainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fid_features=2048,\n",
    "        in_channels=3,\n",
    "        num_hiddens=32,\n",
    "        num_downsampling_layers=4,\n",
    "        num_residual_layers=4,\n",
    "        num_residual_hiddens=64,\n",
    "        embedding_dim=16,\n",
    "        num_embeddings=256,\n",
    "        use_ema=True,\n",
    "        decay=0.99,\n",
    "        epsilon=1e-5,\n",
    "        beta=1,\n",
    "        lr=2e-4,\n",
    "        batch_size=64,\n",
    "    ):\n",
    "        super(VQVAE_Trainer, self).__init__()\n",
    "        self.model = VQVAE(\n",
    "            in_channels=in_channels,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_downsampling_layers=num_downsampling_layers,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            num_residual_hiddens=num_residual_hiddens,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_embeddings=num_embeddings,\n",
    "            use_ema=use_ema,\n",
    "            decay=decay,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "\n",
    "        self.fid = FrechetInceptionDistance(fid_features)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x, reduction=\"sum\")\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_recon_error\", recon_error)\n",
    "        self.log(\"train_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x, reduction=\"sum\")\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_recon_error\", recon_error)\n",
    "        self.log(\"val_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            self.logger.experiment.log(\n",
    "                {\n",
    "                    \"reconstructed\": wandb.Image(\n",
    "                        torchvision.utils.make_grid(out[\"x_recon\"]),\n",
    "                        caption=f\"Epoch {self.current_epoch}, Step {self.global_step}\",\n",
    "                    ),\n",
    "                    # \"real\": wandb.Image(\n",
    "                    #     torchvision.utils.make_grid(x),\n",
    "                    #     caption=f\"Epoch {self.current_epoch}, Step {self.global_step}\",\n",
    "                    # ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Resize to 299x299\n",
    "            x = F.interpolate(x, size=299)\n",
    "            x_hat = F.interpolate(out[\"x_recon\"], size=299)\n",
    "\n",
    "            # Convert to u8\n",
    "            x = (x * 255).to(torch.uint8)\n",
    "            x_hat = (x_hat * 255).to(torch.uint8)\n",
    "\n",
    "            # Compute FID\n",
    "            self.fid.update(x, real=True)\n",
    "            self.fid.update(x_hat, real=False)\n",
    "\n",
    "            self.log(\"fid\", self.fid.compute())\n",
    "            self.fid.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/train\",\n",
    "                transform=torchvision.transforms.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.Resize((256, 256)),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=16,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\",\n",
    "                transform=torchvision.transforms.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.Resize((256, 256)),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            persistent_workers=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init()\n",
    "artifact = run.use_artifact('karanravindra/vq-vae/model-do5i415z:v0', type='model')\n",
    "artifact_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dir = \"artifacts/model-do5i415z:v0\"\n",
    "\n",
    "model = VQVAE_Trainer.load_from_checkpoint(artifact_dir+\"/model.ckpt\").requires_grad_(False)\n",
    "model = model.to(device)\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_dataset = torchvision.datasets.ImageFolder(\n",
    "    \"data/celeba_hq/train\",\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((256, 256)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "og_loader = torch.utils.data.DataLoader(og_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    quantized_dataset = []\n",
    "    \n",
    "    for x, _ in tqdm.tqdm(og_loader):\n",
    "        x = x.to(device)\n",
    "        \n",
    "        embed_indexes = model.model.quantize(x)[3]\n",
    "        # print(\"indexes: \", embed_indexes.shape, embed_indexes[0])\n",
    "        \n",
    "        # z_quantized = F.embedding(\n",
    "        #     embed_indexes.view(x.shape[0], 16, 16),\n",
    "        #     model.model.vq.e_i_ts.transpose(0, 1)\n",
    "        # ).permute(0, 3, 1, 2)\n",
    "        # print(\"z_quantized: \", z_quantized.shape, z_quantized[0])\n",
    "        \n",
    "        # img = model.model.decoder(z_quantized)\n",
    "        \n",
    "        # plt.imshow(img[0].permute(1, 2, 0).detach().cpu().clamp(0, 1))\n",
    "        \n",
    "        quantized_dataset.append(embed_indexes)\n",
    "        \n",
    "    quantized_dataset = torch.cat(quantized_dataset)\n",
    "    torch.save(quantized_dataset, \"quantized.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
