{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import tqdm as tqdm\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "\n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** (\n",
    "        (\n",
    "            torch.arange(\n",
    "                start=0,\n",
    "                end=temb_dim // 2,\n",
    "                dtype=torch.float32,\n",
    "                device=time_steps.device,\n",
    "            )\n",
    "            / (temb_dim // 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample using 2x2 average pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        t_emb_dim,\n",
    "        down_sample=True,\n",
    "        num_heads=2,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels if i == 0 else out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.SiLU(), nn.Linear(t_emb_dim, out_channels))\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(4, out_channels) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels if i == 0 else out_channels, out_channels, kernel_size=1\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = (\n",
    "            nn.Conv2d(out_channels, out_channels, 4, 2, 1)\n",
    "            if self.down_sample\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "            # Attention block of Unet\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=2, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels if i == 0 else out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.SiLU(), nn.Linear(t_emb_dim, out_channels))\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(4, out_channels) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels if i == 0 else out_channels, out_channels, kernel_size=1\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "\n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        t_emb_dim,\n",
    "        up_sample=True,\n",
    "        num_heads=2,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels if i == 0 else out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.SiLU(), nn.Linear(t_emb_dim, out_channels))\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(4, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(\n",
    "                        out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(4, out_channels) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels if i == 0 else out_channels, out_channels, kernel_size=1\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = (\n",
    "            nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 4, 2, 1)\n",
    "            if self.up_sample\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Unet model comprising\n",
    "    Down blocks, Midblocks and Uplocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        im_channels = model_config[\"im_channels\"]\n",
    "        self.down_channels = model_config[\"down_channels\"]\n",
    "        self.mid_channels = model_config[\"mid_channels\"]\n",
    "        self.t_emb_dim = model_config[\"time_emb_dim\"]\n",
    "        self.down_sample = model_config[\"down_sample\"]\n",
    "        self.num_down_layers = model_config[\"num_down_layers\"]\n",
    "        self.num_mid_layers = model_config[\"num_mid_layers\"]\n",
    "        self.num_up_layers = model_config[\"num_up_layers\"]\n",
    "\n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "\n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "        )\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1)\n",
    "        )\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(\n",
    "                DownBlock(\n",
    "                    self.down_channels[i],\n",
    "                    self.down_channels[i + 1],\n",
    "                    self.t_emb_dim,\n",
    "                    down_sample=self.down_sample[i],\n",
    "                    num_layers=self.num_down_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(\n",
    "                MidBlock(\n",
    "                    self.mid_channels[i],\n",
    "                    self.mid_channels[i + 1],\n",
    "                    self.t_emb_dim,\n",
    "                    num_layers=self.num_mid_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels) - 1)):\n",
    "            self.ups.append(\n",
    "                UpBlock(\n",
    "                    self.down_channels[i] * 2,\n",
    "                    self.down_channels[i - 1] if i != 0 else 16,\n",
    "                    self.t_emb_dim,\n",
    "                    up_sample=self.down_sample[i],\n",
    "                    num_layers=self.num_up_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(4, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.cass_embed = nn.Embedding(2, self.t_emb_dim)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        out = self.conv_in(x)\n",
    "        # B x C1 x H x W\n",
    "\n",
    "        y_emb = self.cass_embed(y.long())\n",
    "\n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb) + y_emb\n",
    "\n",
    "        down_outs = []\n",
    "\n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "\n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "\n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, scheduler, device, img_size=(3, 64, 64)):\n",
    "        \"\"\"Sample stepwise by going backward one timestep at a time.\n",
    "        We save the x0 predictions\n",
    "        \"\"\"\n",
    "        xt = torch.randn(8, *img_size, device=device)\n",
    "        y = torch.cat([torch.zeros(4), torch.ones(4)]).to(device)\n",
    "        for i in tqdm.tqdm(\n",
    "            reversed(range(scheduler.num_timesteps)),\n",
    "            desc=\"Sampling Model\",\n",
    "            total=scheduler.num_timesteps,\n",
    "            unit=\"timesteps\",\n",
    "            leave=True,\n",
    "        ):\n",
    "            # Get prediction of noise\n",
    "            noise_pred = self.forward(xt, y, torch.as_tensor(8 * [i], device=device))\n",
    "\n",
    "            # Use scheduler to get x0 and xt-1\n",
    "            xt, x0_pred = scheduler.sample_prev_timestep(\n",
    "                xt, noise_pred, torch.as_tensor(i, device=device)\n",
    "            )\n",
    "\n",
    "        # return final image\n",
    "        # Save x0\n",
    "        ims = torch.clamp(x0_pred, 0.0, 1.0).detach().cpu()\n",
    "        return ims\n",
    "\n",
    "\n",
    "class LinearNoiseScheduler:\n",
    "    \"\"\"\n",
    "    Class for the linear noise scheduler that is used in DDPM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add_noise(self, original, noise, t):\n",
    "        r\"\"\"\n",
    "        Forward method for diffusion\n",
    "        :param original: Image on which noise is to be applied\n",
    "        :param noise: Random Noise Tensor (from normal dist)\n",
    "        :param t: timestep of the forward process of shape -> (B,)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "\n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(\n",
    "            batch_size\n",
    "        )\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(\n",
    "            original.device\n",
    "        )[t].reshape(batch_size)\n",
    "\n",
    "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "\n",
    "        # Apply and Return Forward process equation\n",
    "        return (\n",
    "            sqrt_alpha_cum_prod.to(original.device) * original\n",
    "            + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        r\"\"\"\n",
    "            Use the noise prediction by model to get\n",
    "            xt-1 using xt and the noise predicted\n",
    "        :param xt: current timestep sample\n",
    "        :param noise_pred: model noise prediction\n",
    "        :param t: current timestep we are at\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x0 = (\n",
    "            xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)\n",
    "        ) / torch.sqrt(self.alpha_cum_prod.to(xt.device)[t])\n",
    "        x0 = torch.clamp(x0, -1.0, 1.0)\n",
    "\n",
    "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (\n",
    "            self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t]\n",
    "        )\n",
    "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
    "\n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "        else:\n",
    "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (\n",
    "                1.0 - self.alpha_cum_prod.to(xt.device)[t]\n",
    "            )\n",
    "            variance = variance * self.betas.to(xt.device)[t]\n",
    "            sigma = variance**0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "\n",
    "            # OR\n",
    "            # variance = self.betas[t]\n",
    "            # sigma = variance ** 0.5\n",
    "            # z = torch.randn(xt.shape).to(xt.device)\n",
    "            return mean + sigma * z, x0\n",
    "\n",
    "\n",
    "class DDPM(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_steps=1000,\n",
    "        lr=1e-4,\n",
    "        batch_size=8,\n",
    "        fid_features=2048,\n",
    "        b1=1e-4,\n",
    "        b2=0.02,\n",
    "    ):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.model = Unet(\n",
    "            {\n",
    "                \"im_channels\": 3,\n",
    "                \"im_size\": 64,\n",
    "                \"down_channels\": [16, 32, 64, 128],\n",
    "                \"mid_channels\": [128, 128, 64],\n",
    "                \"down_sample\": [True, True, False],\n",
    "                \"time_emb_dim\": 64,\n",
    "                \"num_down_layers\": 2,\n",
    "                \"num_mid_layers\": 2,\n",
    "                \"num_up_layers\": 2,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = LinearNoiseScheduler(diffusion_steps, b1, b2)\n",
    "        # self.fid = FrechetInceptionDistance(feature=fid_features)\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = F.mse_loss\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert True, \"Forward not implemented\"\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        t = torch.randint(\n",
    "            0, self.scheduler.num_timesteps, (x.size(0),), device=x.device\n",
    "        )\n",
    "        noise = torch.randn_like(x, device=x.device)\n",
    "        x_noisy = self.scheduler.add_noise(x, noise, t)\n",
    "\n",
    "        # Forward Pass\n",
    "        x_pred = self.model(x_noisy, y, t)\n",
    "        loss = self.criterion(x_pred, noise)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        ims = self.model.sample(self.scheduler, x.device)\n",
    "        self.logger.experiment.log(\n",
    "            {\n",
    "                \"Generated Images\": wandb.Image(\n",
    "                    torchvision.utils.make_grid(ims, normalize=True),\n",
    "                    caption=f\"Epoch {self.current_epoch}, Step {self.global_step}\",\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # # Resize to 299x299\n",
    "        # x = F.interpolate(x, size=299, mode=\"bilinear\")\n",
    "        # x_fake = F.interpolate(x_fake, size=299, mode=\"bilinear\")\n",
    "\n",
    "        # # Make u8\n",
    "        # x = (x * 255).to(torch.uint8)\n",
    "        # x_fake = (x_fake * 255).to(torch.uint8)\n",
    "\n",
    "        # # Compute FID\n",
    "        # self.fid.update(x, True)\n",
    "        # self.fid.update(x_fake, False)\n",
    "\n",
    "        # self.log(\"fid\", self.fid.compute())\n",
    "\n",
    "        # self.logger.experiment.log(\n",
    "        #     {\n",
    "        #         \"Generated Images\": wandb.Image(\n",
    "        #             torchvision.utils.make_grid(x_fake, nrow=8),\n",
    "        #             caption=f\"Epoch {self.current_epoch}, Step {self.global_step}\",\n",
    "        #         )\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/train\",\n",
    "                transform=torchvision.transforms.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.Resize(64),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\",\n",
    "                transform=torchvision.transforms.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.Resize(64),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            persistent_workers=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DDPM()\n",
    "\n",
    "# load model from checkpoint\n",
    "model = DDPM.load_from_checkpoint(\"projects/image-generation/7-ddpm/logs/ddpm-faceforger/hi5jx35n/checkpoints/epoch=16-step=7446.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Model:   0%|          | 1/1000 [00:15<4:12:19, 15.15s/timesteps]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sample from model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 459\u001b[0m, in \u001b[0;36mUnet.sample\u001b[0;34m(self, scheduler, device, img_size)\u001b[0m\n\u001b[1;32m    450\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m), torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m4\u001b[39m)])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(scheduler\u001b[38;5;241m.\u001b[39mnum_timesteps)),\n\u001b[1;32m    453\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampling Model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m ):\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# Get prediction of noise\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Use scheduler to get x0 and xt-1\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     xt, x0_pred \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39msample_prev_timestep(\n\u001b[1;32m    463\u001b[0m         xt, noise_pred, torch\u001b[38;5;241m.\u001b[39mas_tensor(i, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    464\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[2], line 426\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, y, t)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, down \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdowns):\n\u001b[1;32m    425\u001b[0m     down_outs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[0;32m--> 426\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# out B x C4 x H/4 x W/4\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmids:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 126\u001b[0m, in \u001b[0;36mDownBlock.forward\u001b[0;34m(self, x, t_emb)\u001b[0m\n\u001b[1;32m    124\u001b[0m in_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_norms[i](in_attn)\n\u001b[1;32m    125\u001b[0m in_attn \u001b[38;5;241m=\u001b[39m in_attn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m out_attn, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattentions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_attn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m out_attn \u001b[38;5;241m=\u001b[39m out_attn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, channels, h, w)\n\u001b[1;32m    128\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m out_attn\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/functional.py:5457\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average_attn_weights:\n\u001b[1;32m   5455\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m attn_output_weights\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 5457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5458\u001b[0m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n\u001b[1;32m   5459\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5460\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m attn_output_weights\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sample from model\n",
    "imgs = model.model.sample(model.scheduler, \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(torchvision.utils.make_grid(imgs, nrow=8).permute(1, 2, 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
