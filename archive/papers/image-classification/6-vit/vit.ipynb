{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 60000 samples\n",
      "Testing with 60000 samples\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.QMNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.QMNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=60, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=60, shuffle=False)\n",
    "\n",
    "print(f\"Training with {len(train_dataset)} samples\")\n",
    "print(f\"Testing with {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 4\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH\n",
    "IMAGE_CHANNELS = 1\n",
    "EMBEDDING_DIMS = IMAGE_CHANNELS * PATCH_SIZE**2\n",
    "NUM_OF_PATCHES = int((IMAGE_WIDTH * IMAGE_HEIGHT) / PATCH_SIZE**2)\n",
    "\n",
    "assert IMAGE_WIDTH % PATCH_SIZE == 0 and IMAGE_HEIGHT % PATCH_SIZE ==0 , print(\"Image Width is not divisible by patch size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 50, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=2)\n",
    "        self.class_token_embeddings = nn.Parameter(torch.rand((60, 1, EMBEDDING_DIMS), requires_grad=True))\n",
    "        self.position_embeddings = nn.Parameter(torch.rand((1, NUM_OF_PATCHES + 1, EMBEDDING_DIMS), requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat((self.class_token_embeddings, self.flatten_layer(self.conv_layer(x).permute((0, 2, 3, 1)))), dim=1) + self.position_embeddings\n",
    "        return output\n",
    "    \n",
    "patch_embedding_layer = PatchEmbeddingLayer(in_channels=IMAGE_CHANNELS, patch_size=PATCH_SIZE, embedding_dim=IMAGE_CHANNELS * PATCH_SIZE ** 2)\n",
    "\n",
    "patch_embeddings = patch_embedding_layer(next(iter(train_loader))[0])\n",
    "patch_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input Patch Embeddings => [60, 50, 16] <= [batch_size, num_patches+1, embedding_dims ]\n",
      "Shape of the output from MSA Block => [60, 50, 16] <= [batch_size, num_patches+1, embedding_dims ]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dims=768,  # Hidden Size D in the ViT Paper Table 1\n",
    "        num_heads=12,  # Heads in the ViT Paper Table 1\n",
    "        attn_dropout=0.0,  # Default to Zero as there is no dropout for the the MSA Block as per the ViT Paper\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.num_head = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape=embedding_dims)\n",
    "\n",
    "        self.multiheadattention = nn.MultiheadAttention(\n",
    "            num_heads=num_heads,\n",
    "            embed_dim=embedding_dims,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm(x)\n",
    "        output, _ = self.multiheadattention(query=x, key=x, value=x, need_weights=False)\n",
    "        return output\n",
    "\n",
    "\n",
    "multihead_self_attention_block = MultiHeadSelfAttentionBlock(\n",
    "    embedding_dims=EMBEDDING_DIMS, num_heads=2\n",
    ")\n",
    "print(\n",
    "    f\"Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]\"\n",
    ")\n",
    "print(\n",
    "    f\"Shape of the output from MSA Block => {list(multihead_self_attention_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningPerceptronBlock(nn.Module):\n",
    "  def __init__(self, embedding_dims, mlp_size, mlp_dropout):\n",
    "    super().__init__()\n",
    "    self.embedding_dims = embedding_dims\n",
    "    self.mlp_size = mlp_size\n",
    "    self.dropout = mlp_dropout\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(normalized_shape = embedding_dims)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features = embedding_dims, out_features = mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p = mlp_dropout),\n",
    "        nn.Linear(in_features = mlp_size, out_features = embedding_dims),\n",
    "        nn.Dropout(p = mlp_dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.mlp(self.layernorm(x))\n",
    "\n",
    "mlp_block = MachineLearningPerceptronBlock(embedding_dims = EMBEDDING_DIMS,\n",
    "                                           mlp_size = 3072,\n",
    "                                           mlp_dropout = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input Patch Embeddings => [60, 50, 16] <= [batch_size, num_patches+1, embedding_dims ]\n",
      "Shape of the output from Transformer Block => [60, 50, 16] <= [batch_size, num_patches+1, embedding_dims ]\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dims=768,\n",
    "        mlp_dropout=0.1,\n",
    "        attn_dropout=0.0,\n",
    "        mlp_size=3072,\n",
    "        num_heads=12,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.msa_block = MultiHeadSelfAttentionBlock(\n",
    "            embedding_dims=embedding_dims,\n",
    "            num_heads=num_heads,\n",
    "            attn_dropout=attn_dropout,\n",
    "        )\n",
    "\n",
    "        self.mlp_block = MachineLearningPerceptronBlock(\n",
    "            embedding_dims=embedding_dims,\n",
    "            mlp_size=mlp_size,\n",
    "            mlp_dropout=mlp_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(x) + x\n",
    "        x = self.mlp_block(x) + x\n",
    "\n",
    "        return x\n",
    "    \n",
    "transformer_block = TransformerBlock(embedding_dims = EMBEDDING_DIMS,\n",
    "                                     mlp_dropout = 0.1,\n",
    "                                     attn_dropout=0.0,\n",
    "                                     mlp_size = 3072,\n",
    "                                     num_heads = 2)\n",
    "\n",
    "print(f'Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n",
    "print(f'Shape of the output from Transformer Block => {list(transformer_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=28,\n",
    "        in_channels=1,\n",
    "        patch_size=7,\n",
    "        embedding_dims=16,\n",
    "        num_transformer_layers=1,  # from table 1 above\n",
    "        mlp_dropout=0.1,\n",
    "        attn_dropout=0.0,\n",
    "        mlp_size=64,\n",
    "        num_heads=2,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding_layer = PatchEmbeddingLayer(\n",
    "            in_channels=in_channels, patch_size=patch_size, embedding_dim=embedding_dims\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embedding_dims=embedding_dims,\n",
    "                    mlp_dropout=mlp_dropout,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                    mlp_size=mlp_size,\n",
    "                    num_heads=num_heads,\n",
    "                )\n",
    "                for _ in range(num_transformer_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dims),\n",
    "            nn.Linear(in_features=embedding_dims, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(\n",
    "            self.transformer_encoder(self.patch_embedding_layer(x))[:, 0]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT()\n",
    "criterion = F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 54.65batch/s, Loss: 0.33524078130722046]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 65.59batch/s, Loss: 0.31849202513694763]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 64.51batch/s, Loss: 0.6403032541275024]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 64.79batch/s, Loss: 0.2636517286300659]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 65.74batch/s, Loss: 0.3237418234348297]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 66.40batch/s, Loss: 0.3689804971218109]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 65.39batch/s, Loss: 0.5953991413116455]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 65.42batch/s, Loss: 0.5439376831054688]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 65.20batch/s, Loss: 0.28704044222831726]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 66.13batch/s, Loss: 0.2872060537338257]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    with tqdm.tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for data, target in tepoch:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix_str(f\"Loss: {loss.item()}\")\n",
    "            \n",
    "        # correct = 0\n",
    "        # total = 0\n",
    "        # with torch.no_grad():\n",
    "        #     for data, target in test_loader:\n",
    "        #         data, target = data.to(device), target.to(device)\n",
    "                \n",
    "        #         outputs = model(data)\n",
    "        #         _, predicted = torch.max(outputs.data, 1)\n",
    "        #         total += target.size(0)\n",
    "        #         correct += (predicted == target).sum().item()\n",
    "                \n",
    "        # print(f\"Accuracy: {correct / total}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (17) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m data, target \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m target\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embedding_layer(x))[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_token_embeddings, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten_layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_layer(x)\u001b[39m.\u001b[39;49mpermute((\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m)))), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embeddings\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hv2y3mbnxreke2pye041pr6c.studio.lightning.ai/teamspace/studios/this_studio/projects/image-classification/6-vit/vit.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (17) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    with tqdm.tqdm(test_loader, unit=\"batch\") as pbar:\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            pbar.set_postfix_str(f\"Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90065\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
