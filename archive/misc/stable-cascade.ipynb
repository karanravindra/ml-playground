{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n",
    "\n",
    "prior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", variant=\"bf16\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "decoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", variant=\"bf16\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A high quality, creative, portrait of a dog, against a plain background.\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "for i in tqdm.tqdm(range(0, 100, 2)):\n",
    "    # prior.enable_model_cpu_offload()\n",
    "    prior_output = prior(\n",
    "        prompt=prompt,\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=4.0,\n",
    "        num_images_per_prompt=2,\n",
    "        num_inference_steps=20\n",
    "    )\n",
    "\n",
    "    # decoder.enable_model_cpu_offload()\n",
    "    decoder_output = decoder(\n",
    "        image_embeddings=prior_output.image_embeddings.to(torch.float16),\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=0.0,\n",
    "        output_type=\"pil\",\n",
    "        num_inference_steps=10\n",
    "    )\n",
    "    \n",
    "    # Save the image\n",
    "    for idx, image in enumerate(decoder_output.images):\n",
    "        image.save(f\"projects/custom/dataset/dog/{i+idx}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = [\"cat\", \"giraffe\", \"tiger\", \"bear\", \"penguin\", \"panda\"]\n",
    "for animal in animals:\n",
    "    prompt = f\"A high quality, creative, portrait of a {animal}, against a plain background.\"\n",
    "    negative_prompt = \"\"\n",
    "\n",
    "    for i in tqdm.tqdm(range(0, 100, 2)):\n",
    "        # prior.enable_model_cpu_offload()\n",
    "        prior_output = prior(\n",
    "            prompt=prompt,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            negative_prompt=negative_prompt,\n",
    "            guidance_scale=4.0,\n",
    "            num_images_per_prompt=2,\n",
    "            num_inference_steps=20\n",
    "        )\n",
    "\n",
    "        # decoder.enable_model_cpu_offload()\n",
    "        decoder_output = decoder(\n",
    "            image_embeddings=prior_output.image_embeddings.to(torch.float16),\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            guidance_scale=0.0,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=10\n",
    "        )\n",
    "        \n",
    "        # Save the image\n",
    "        for idx, image in enumerate(decoder_output.images):\n",
    "            image.save(f\"projects/custom/dataset/{animal}/{i+idx}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 858,411 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = diffusers.VQModel(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    down_block_types=(\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"),\n",
    "    up_block_types=(\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"),\n",
    "    block_out_channels=(32, 64),\n",
    "    layers_per_block=2,\n",
    "    latent_channels=4,\n",
    "    sample_size=256,\n",
    "    num_vq_embeddings=256,\n",
    "    vq_embed_dim=64,\n",
    "    norm_num_groups=16,\n",
    ")\n",
    "print(f\"Model has {model.num_parameters():,} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(sample=tensor([[[[ 1.0439e+00, -4.9478e-01, -5.3723e-01,  ..., -4.6625e-01,\n",
       "           -8.5494e-01, -7.6728e-01],\n",
       "          [ 5.8595e-01,  1.0339e+00,  6.6085e-03,  ...,  8.0054e-01,\n",
       "            1.3276e+00,  8.7352e-01],\n",
       "          [ 1.6277e+00,  2.4285e+00,  7.0050e-01,  ...,  6.0781e-01,\n",
       "            6.3180e-01,  1.3622e-01],\n",
       "          ...,\n",
       "          [ 7.7525e-01,  8.2628e-01,  1.6002e+00,  ...,  4.8426e-01,\n",
       "           -4.3053e-01,  3.0097e-01],\n",
       "          [ 1.0023e+00,  1.2221e+00,  1.7539e+00,  ...,  1.7192e-01,\n",
       "            1.9404e-01, -7.6295e-03],\n",
       "          [ 6.2281e-01,  6.0951e-01,  5.6773e-01,  ..., -1.1793e-02,\n",
       "           -4.7666e-02, -1.5319e-01]],\n",
       "\n",
       "         [[-7.6515e-01, -7.5619e-01, -1.1668e+00,  ...,  8.6761e-01,\n",
       "            2.1760e-01,  3.0377e-01],\n",
       "          [-1.0732e+00, -1.0866e+00, -4.8508e-01,  ...,  2.7568e-01,\n",
       "            2.5070e-01,  1.9509e-01],\n",
       "          [-2.5499e+00, -8.2628e-01, -7.6318e-01,  ..., -1.3304e-01,\n",
       "            2.0589e-01,  2.7565e-01],\n",
       "          ...,\n",
       "          [-2.9434e-01, -6.5083e-01, -6.8949e-01,  ..., -9.0893e-01,\n",
       "           -7.9297e-01, -5.1442e-01],\n",
       "          [-2.3351e-01, -7.3815e-01, -4.0788e-01,  ..., -1.6328e+00,\n",
       "           -8.4253e-01, -9.6899e-01],\n",
       "          [ 2.5880e-01, -1.2264e-01, -6.2343e-01,  ..., -7.1998e-01,\n",
       "           -4.9071e-01, -5.8405e-01]],\n",
       "\n",
       "         [[ 1.0735e+00,  1.3817e+00,  9.6019e-01,  ...,  3.4416e-01,\n",
       "            8.1896e-02, -6.8176e-02],\n",
       "          [ 1.6205e+00,  1.4467e+00,  7.0898e-02,  ...,  1.3302e+00,\n",
       "            9.4600e-01,  9.6271e-02],\n",
       "          [ 1.5719e+00,  2.1302e+00,  1.0793e+00,  ...,  8.0323e-01,\n",
       "            7.7471e-01,  2.7173e-01],\n",
       "          ...,\n",
       "          [ 1.1631e+00,  9.5645e-01,  1.0007e+00,  ..., -2.7997e-01,\n",
       "            7.4593e-01,  2.1665e-01],\n",
       "          [ 6.3723e-01,  1.6693e-01,  1.2795e+00,  ..., -1.3143e-02,\n",
       "            7.0004e-01,  2.6479e-01],\n",
       "          [ 5.3382e-01,  3.9644e-01,  3.6747e-01,  ...,  1.3112e-04,\n",
       "            4.2364e-01,  4.5375e-01]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.randn(1, 3, 256, 256), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
