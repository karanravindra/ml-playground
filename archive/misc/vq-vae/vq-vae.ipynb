{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import tqdm as tqdm\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        layers = []\n",
    "        for i in range(num_residual_layers):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=num_hiddens,\n",
    "                        out_channels=num_residual_hiddens,\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=num_residual_hiddens,\n",
    "                        out_channels=num_hiddens,\n",
    "                        kernel_size=1,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = h + layer(h)\n",
    "\n",
    "        # ResNet V1-style.\n",
    "        return torch.relu(h)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        num_hiddens,\n",
    "        num_downsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
    "        # off with a ReLU.\n",
    "        conv = nn.Sequential()\n",
    "        for downsampling_layer in range(num_downsampling_layers):\n",
    "            if downsampling_layer == 0:\n",
    "                out_channels = num_hiddens // 2\n",
    "            elif downsampling_layer == 1:\n",
    "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
    "\n",
    "            else:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
    "\n",
    "            conv.add_module(\n",
    "                f\"down{downsampling_layer}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            )\n",
    "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
    "\n",
    "        conv.add_module(\n",
    "            \"final_conv\",\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_hiddens,\n",
    "                out_channels=num_hiddens,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "        self.conv = conv\n",
    "        self.residual_stack = ResidualStack(\n",
    "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        return self.residual_stack(h)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_hiddens,\n",
    "        num_upsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "        out,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=num_hiddens,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.residual_stack = ResidualStack(\n",
    "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
    "        )\n",
    "        upconv = nn.Sequential()\n",
    "        for upsampling_layer in range(num_upsampling_layers):\n",
    "            if upsampling_layer < num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
    "\n",
    "            elif upsampling_layer == num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
    "\n",
    "            elif upsampling_layer > num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens // 2, out)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid upsampling layer: {upsampling_layer}. In Encoder.\"\n",
    "                )\n",
    "\n",
    "            upconv.add_module(\n",
    "                f\"up{upsampling_layer}\",\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            )\n",
    "            if upsampling_layer < num_upsampling_layers - 1:\n",
    "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
    "\n",
    "        self.upconv = upconv\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = self.residual_stack(h)\n",
    "        x_recon = self.upconv(h)\n",
    "        return F.sigmoid(x_recon)\n",
    "\n",
    "\n",
    "class SonnetExponentialMovingAverage(nn.Module):\n",
    "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
    "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
    "    # of \"Neural Discrete Representation Learning\".\n",
    "    def __init__(self, decay, shape):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        self.counter = 0\n",
    "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
    "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
    "\n",
    "    def update(self, value):\n",
    "        self.counter += 1\n",
    "        with torch.no_grad():\n",
    "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
    "            self.average = self.hidden / (1 - self.decay**self.counter)\n",
    "\n",
    "    def __call__(self, value):\n",
    "        self.update(value)\n",
    "        return self.average\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
    "        super().__init__()\n",
    "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
    "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.use_ema = use_ema\n",
    "        # Weight for the exponential moving average.\n",
    "        self.decay = decay\n",
    "        # Small constant to avoid numerical instability in embedding updates.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Dictionary embeddings.\n",
    "        limit = 3**0.5\n",
    "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_(\n",
    "            -limit, limit\n",
    "        )\n",
    "        if use_ema:\n",
    "            self.register_buffer(\"e_i_ts\", e_i_ts)\n",
    "        else:\n",
    "            self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
    "\n",
    "        # Exponential moving average of the cluster counts.\n",
    "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
    "        # Exponential moving average of the embeddings.\n",
    "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
    "        distances = (\n",
    "            (flat_x**2).sum(1, keepdim=True)\n",
    "            - 2 * flat_x @ self.e_i_ts\n",
    "            + (self.e_i_ts**2).sum(0, keepdim=True)\n",
    "        )\n",
    "        encoding_indices = distances.argmin(1)\n",
    "        quantized_x = F.embedding(\n",
    "            encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)\n",
    "        ).permute(0, 3, 1, 2)\n",
    "\n",
    "        # See second term of Equation (3).\n",
    "        if not self.use_ema:\n",
    "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
    "        else:\n",
    "            dictionary_loss = None\n",
    "\n",
    "        # See third term of Equation (3).\n",
    "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
    "        # Straight-through gradient. See Section 3.2.\n",
    "        quantized_x = x + (quantized_x - x).detach()\n",
    "\n",
    "        if self.use_ema and self.training:\n",
    "            with torch.no_grad():\n",
    "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
    "\n",
    "                # Cluster counts.\n",
    "                encoding_one_hots = F.one_hot(\n",
    "                    encoding_indices, self.num_embeddings\n",
    "                ).type(flat_x.dtype)\n",
    "                n_i_ts = encoding_one_hots.sum(0)\n",
    "                # Updated exponential moving average of the cluster counts.\n",
    "                # See Equation (6).\n",
    "                self.N_i_ts(n_i_ts)\n",
    "\n",
    "                # Exponential moving average of the embeddings. See Equation (7).\n",
    "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
    "                self.m_i_ts(embed_sums)\n",
    "\n",
    "                # This is kind of weird.\n",
    "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
    "                # and Equation (8).\n",
    "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
    "                N_i_ts_stable = (\n",
    "                    (self.N_i_ts.average + self.epsilon)\n",
    "                    / (N_i_ts_sum + self.num_embeddings * self.epsilon)\n",
    "                    * N_i_ts_sum\n",
    "                )\n",
    "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
    "\n",
    "        return (\n",
    "            quantized_x,\n",
    "            dictionary_loss,\n",
    "            commitment_loss,\n",
    "            encoding_indices.view(x.shape[0], -1),\n",
    "        )\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_hiddens,\n",
    "        num_downsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "        embedding_dim,\n",
    "        num_embeddings,\n",
    "        use_ema,\n",
    "        decay,\n",
    "        epsilon,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            in_channels,\n",
    "            num_hiddens,\n",
    "            num_downsampling_layers,\n",
    "            num_residual_layers,\n",
    "            num_residual_hiddens,\n",
    "        )\n",
    "        self.pre_vq_conv = nn.Conv2d(\n",
    "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
    "        )\n",
    "        self.vq = VectorQuantizer(\n",
    "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim,\n",
    "            num_hiddens,\n",
    "            num_downsampling_layers,\n",
    "            num_residual_layers,\n",
    "            num_residual_hiddens,\n",
    "            out_channels,\n",
    "        )\n",
    "\n",
    "    def quantize(self, x):\n",
    "        z = self.pre_vq_conv(self.encoder(x))\n",
    "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
    "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = (\n",
    "            self.quantize(x)\n",
    "        )\n",
    "        x_recon = self.decoder(z_quantized)\n",
    "        return {\n",
    "            \"dictionary_loss\": dictionary_loss,\n",
    "            \"commitment_loss\": commitment_loss,\n",
    "            \"x_recon\": x_recon,\n",
    "            \"encoding_indices\": encoding_indices,\n",
    "        }\n",
    "\n",
    "\n",
    "class VQVAE_Trainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size=64,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        num_hiddens=64,\n",
    "        num_downsampling_layers=2,\n",
    "        num_residual_layers=2,\n",
    "        num_residual_hiddens=128,\n",
    "        embedding_dim=4,  # 32, 64, 128, 256\n",
    "        num_embeddings=64,  # 256, 512, 1024, 2048\n",
    "        use_ema=True,\n",
    "        decay=0.99,\n",
    "        epsilon=1e-5,\n",
    "        beta=0.25,\n",
    "        lr=2e-4,\n",
    "        weight_decay=0.0,\n",
    "        fid_features=2048,\n",
    "        batch_size=64,  # 128\n",
    "        dataset=\"celeba_hq\",\n",
    "    ):\n",
    "        super(VQVAE_Trainer, self).__init__()\n",
    "        self.model = VQVAE(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_downsampling_layers=num_downsampling_layers,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            num_residual_hiddens=num_residual_hiddens,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_embeddings=num_embeddings,\n",
    "            use_ema=use_ema,\n",
    "            decay=decay,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "\n",
    "        self.beta = beta\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x)\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        if out[\"dictionary_loss\"] is not None:\n",
    "            loss += out[\"dictionary_loss\"]\n",
    "            self.log(\"train_dictionary_loss\", out[\"dictionary_loss\"])\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_recon_error\", recon_error)\n",
    "        self.log(\"train_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x)\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        if out[\"dictionary_loss\"] is not None:\n",
    "            loss += out[\"dictionary_loss\"]\n",
    "            self.log(\"val_dictionary_loss\", out[\"dictionary_loss\"])\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_recon_error\", recon_error)\n",
    "        self.log(\"val_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            if self.global_step == 0 and batch_idx == 0:\n",
    "                self.logger.experiment.log(\n",
    "                    {\n",
    "                        \"original\": wandb.Image(\n",
    "                            torchvision.utils.make_grid(x[:64], nrow=8),\n",
    "                            caption=\"Real Image\",\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            self.logger.experiment.log(\n",
    "                {\n",
    "                    \"reconstructed\": wandb.Image(\n",
    "                        torchvision.utils.make_grid(out[\"x_recon\"][:64], nrow=8),\n",
    "                        caption=f\"Step {self.global_step}\",\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def on_test_start(self):\n",
    "        self.fid = FrechetInceptionDistance(self.hparams.fid_features).cpu()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        # Resize to 299x299\n",
    "        x = F.interpolate(x, size=299)\n",
    "        x_hat = F.interpolate(out[\"x_recon\"], size=299)\n",
    "\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "            x_hat = x_hat.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Convert to uint8\n",
    "        x = (x * 255).to(torch.uint8).cpu()\n",
    "        x_hat = (x_hat * 255).to(torch.uint8).cpu()\n",
    "\n",
    "        # Compute FID\n",
    "        self.fid.update(x, real=True)\n",
    "        self.fid.update(x_hat, real=False)\n",
    "\n",
    "        fid_score = self.fid.compute()\n",
    "        self.log(\"fid_score\", fid_score)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            amsgrad=True,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    (\n",
    "                        torchvision.transforms.Grayscale()\n",
    "                        if self.hparams.in_channels == 1\n",
    "                        else torchvision.transforms.Lambda(lambda x: x)\n",
    "                    ),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=True, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=True, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/train\", transform=transform\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=False, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=False, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\", transform=transform\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=False, transform=transform, download=True\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 16))\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=False, transform=transform, download=True\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 16))\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\", transform=transform\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 4))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae = VQVAE_Trainer.load_from_checkpoint(\"vqvae/logs/vq-vae/mnist-32/checkpoints/epoch=10-step=10000.ckpt\")\n",
    "model = vqvae.model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "# Print codebookshape\n",
    "print(model.vq.e_i_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = vqvae.train_dataloader()\n",
    "val_loader = vqvae.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 93.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000])\n",
      "torch.Size([60000, 4, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    classes = []\n",
    "    idxs = []\n",
    "\n",
    "    for x, y in tqdm.tqdm(train_loader):    \n",
    "        x = x.to(\"mps\")\n",
    "\n",
    "        out = model.quantize(x)[0]\n",
    "        \n",
    "        classes.extend(y.tolist())\n",
    "        idxs.extend(out.tolist())\n",
    "        \n",
    "    classes = torch.tensor(classes)\n",
    "    idxs = torch.tensor(idxs)\n",
    "\n",
    "    print(classes.shape)\n",
    "    print(idxs.shape)\n",
    "\n",
    "    # Convert to int dataset and save\n",
    "    dataset = torch.utils.data.TensorDataset(idxs, classes)\n",
    "    torch.save(dataset, \"projects/custom/vq-vae/train_z_embed.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 47.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n",
      "torch.Size([10000, 4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    classes = []\n",
    "    idxs = []\n",
    "\n",
    "    for x, y in tqdm.tqdm(val_loader):    \n",
    "        x = x.to(\"mps\")\n",
    "\n",
    "        out = model.quantize(x)[0]\n",
    "        \n",
    "        classes.extend(y.tolist())\n",
    "        idxs.extend(out.tolist())\n",
    "        \n",
    "    classes = torch.tensor(classes)\n",
    "    idxs = torch.tensor(idxs)\n",
    "\n",
    "    print(classes.shape)\n",
    "    print(idxs.shape)\n",
    "\n",
    "    # Convert to int dataset and save\n",
    "    dataset = torch.utils.data.TensorDataset(idxs, classes)\n",
    "    torch.save(dataset, \"projects/custom/vq-vae/val_z_embed.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.load(\"projects/custom/vq-vae/train_indexed.pt\"),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# Val Loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.load(\"projects/custom/vq-vae/val_indexed.pt\"),\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Replace LSTM with GRU\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to predict each character\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device)\n",
    "        \n",
    "        # GRU output along with new hidden state\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # Reshape output for the fully connected layer\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "# Create an instance of the updated model\n",
    "vocab_size = 2048  # number of unique characters\n",
    "embed_dim = 128   # embedding dimension\n",
    "hidden_dim = 256  # LSTM hidden dimensions\n",
    "num_layers = 4  # number of GRU layers\n",
    "\n",
    "lstm = Model(vocab_size, embed_dim, hidden_dim, num_layers).to(\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(lstm, input_data=torch.zeros((64, 64), dtype=torch.long).to(\"mps\"), depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(lstm.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def decode_idxs(idxs):\n",
    "    print(idxs.shape)\n",
    "    quantized_x = F.embedding(\n",
    "        idxs.view(1, 32, 32), model.vq.e_i_ts.transpose(0, 1)\n",
    "    ).permute(0, 3, 1, 2)\n",
    "    print(quantized_x.shape)\n",
    "    # reconstruct\n",
    "    x_recon = model.decoder(quantized_x)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(torchvision.utils.make_grid(x_recon[:64], nrow=8).cpu().numpy().transpose(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def temperature_sampling(logits, temperature=0.7):\n",
    "    # Scale logits by temperature\n",
    "    scaled_logits = logits / temperature\n",
    "    # Convert logits to probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    # Sample from the probabilities\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def generate(len=1024, temperature=0.8):\n",
    "    # Validation Loop with Temperature Sampling\n",
    "    lstm.eval()\n",
    "    initial_input = torch.randint(0, vocab_size, (1, 1), dtype=torch.long).to(\"mps\")\n",
    "    generated_text = []\n",
    "    hidden = None  # Hidden state initialization\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(len):  # Generate 64 characters\n",
    "            output, hidden = lstm(initial_input, hidden)  # Ensure model accepts and returns hidden state\n",
    "            predicted = temperature_sampling(output[-1], temperature=temperature)\n",
    "            generated_text.append(predicted)\n",
    "            initial_input = predicted.unsqueeze(0)\n",
    "\n",
    "    generated = torch.stack(generated_text).squeeze()\n",
    "\n",
    "    decode_idxs(generated)\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   7%|▋         | 29/438 [07:05<1:39:58, 14.67s/it, Loss: 6.1464]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)  \u001b[38;5;66;03m# Loss calculation between outputs and shifted targets\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(train_loader) as pbar:\n",
    "        for x, y in pbar:\n",
    "            x = x.to(\"mps\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Encode sequence and prepare inputs and targets\n",
    "            inputs = x[:, :-1]  # All characters except the last\n",
    "            targets = x[:, 1:].flatten()  # All characters except the first, flattened for loss calculation\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = lstm(inputs)  # Outputs now includes hidden states which are ignored during training\n",
    "            loss = criterion(outputs, targets)  # Loss calculation between outputs and shifted targets\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
