{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import tqdm as tqdm\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "\"\"\"\n",
    "References: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/modules/diffusionmodules/model.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# swish\n",
    "def nonlinearity(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def Normalize(in_channels, num_groups=32):\n",
    "    return torch.nn.GroupNorm(\n",
    "        num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "class Upsample2x(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(F.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
    "\n",
    "\n",
    "class Downsample2x(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=3, stride=2, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(F.pad(x, pad=(0, 1, 0, 1), mode=\"constant\", value=0))\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, *, in_channels, out_channels=None, dropout\n",
    "    ):  # conv_shortcut=False,  # conv_shortcut: always False in VAE\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.norm2 = Normalize(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout) if dropout > 1e-6 else nn.Identity()\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.nin_shortcut = torch.nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "            )\n",
    "        else:\n",
    "            self.nin_shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(F.silu(self.norm1(x), inplace=True))\n",
    "        h = self.conv2(self.dropout(F.silu(self.norm2(h), inplace=True)))\n",
    "        return self.nin_shortcut(x) + h\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.C = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.qkv = torch.nn.Conv2d(\n",
    "            in_channels, 3 * in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.w_ratio = int(in_channels) ** (-0.5)\n",
    "        self.proj_out = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        B, _, H, W = qkv.shape  # should be B,3C,H,W\n",
    "        C = self.C\n",
    "        q, k, v = qkv.reshape(B, 3, C, H, W).unbind(1)\n",
    "\n",
    "        # compute attention\n",
    "        q = q.view(B, C, H * W).contiguous()\n",
    "        q = q.permute(0, 2, 1).contiguous()  # B,HW,C\n",
    "        k = k.view(B, C, H * W).contiguous()  # B,C,HW\n",
    "        w = torch.bmm(q, k).mul_(\n",
    "            self.w_ratio\n",
    "        )  # B,HW,HW    w[B,i,j]=sum_c q[B,i,C]k[B,C,j]\n",
    "        w = F.softmax(w, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.view(B, C, H * W).contiguous()\n",
    "        w = w.permute(0, 2, 1).contiguous()  # B,HW,HW (first HW of k, second of q)\n",
    "        h = torch.bmm(v, w)  # B, C,HW (HW of q) h[B,C,j] = sum_i v[B,C,i] w[B,i,j]\n",
    "        h = h.view(B, C, H, W).contiguous()\n",
    "\n",
    "        return x + self.proj_out(h)\n",
    "\n",
    "\n",
    "def make_attn(in_channels, using_sa=True):\n",
    "    return AttnBlock(in_channels) if using_sa else nn.Identity()\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        ch=128,\n",
    "        ch_mult=(1, 2, 4, 8),\n",
    "        num_res_blocks=2,\n",
    "        dropout=0.0,\n",
    "        in_channels=3,\n",
    "        z_channels,\n",
    "        double_z=False,\n",
    "        using_sa=True,\n",
    "        using_mid_sa=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.downsample_ratio = 2 ** (self.num_resolutions - 1)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # downsampling\n",
    "        self.conv_in = torch.nn.Conv2d(\n",
    "            in_channels, self.ch, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        in_ch_mult = (1,) + tuple(ch_mult)\n",
    "        self.down = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch * in_ch_mult[i_level]\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(\n",
    "                    ResnetBlock(\n",
    "                        in_channels=block_in, out_channels=block_out, dropout=dropout\n",
    "                    )\n",
    "                )\n",
    "                block_in = block_out\n",
    "                if i_level == self.num_resolutions - 1 and using_sa:\n",
    "                    attn.append(make_attn(block_in, using_sa=True))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                down.downsample = Downsample2x(block_in)\n",
    "            self.down.append(down)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(\n",
    "            in_channels=block_in, out_channels=block_in, dropout=dropout\n",
    "        )\n",
    "        self.mid.attn_1 = make_attn(block_in, using_sa=using_mid_sa)\n",
    "        self.mid.block_2 = ResnetBlock(\n",
    "            in_channels=block_in, out_channels=block_in, dropout=dropout\n",
    "        )\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(\n",
    "            block_in,\n",
    "            (2 * z_channels if double_z else z_channels),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsampling\n",
    "        h = self.conv_in(x)\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](h)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                h = self.down[i_level].downsample(h)\n",
    "\n",
    "        # middle\n",
    "        h = self.mid.block_2(self.mid.attn_1(self.mid.block_1(h)))\n",
    "\n",
    "        # end\n",
    "        h = self.conv_out(F.silu(self.norm_out(h), inplace=True))\n",
    "        return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        ch=128,\n",
    "        ch_mult=(1, 2, 4, 8),\n",
    "        num_res_blocks=2,\n",
    "        dropout=0.0,\n",
    "        in_channels=3,  # in_channels: raw img channels\n",
    "        z_channels,\n",
    "        using_sa=True,\n",
    "        using_mid_sa=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
    "        in_ch_mult = (1,) + tuple(ch_mult)\n",
    "        block_in = ch * ch_mult[self.num_resolutions - 1]\n",
    "\n",
    "        # z to block_in\n",
    "        self.conv_in = torch.nn.Conv2d(\n",
    "            z_channels, block_in, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(\n",
    "            in_channels=block_in, out_channels=block_in, dropout=dropout\n",
    "        )\n",
    "        self.mid.attn_1 = make_attn(block_in, using_sa=using_mid_sa)\n",
    "        self.mid.block_2 = ResnetBlock(\n",
    "            in_channels=block_in, out_channels=block_in, dropout=dropout\n",
    "        )\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                block.append(\n",
    "                    ResnetBlock(\n",
    "                        in_channels=block_in, out_channels=block_out, dropout=dropout\n",
    "                    )\n",
    "                )\n",
    "                block_in = block_out\n",
    "                if i_level == self.num_resolutions - 1 and using_sa:\n",
    "                    attn.append(make_attn(block_in, using_sa=True))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample2x(block_in)\n",
    "            self.up.insert(0, up)  # prepend to get consistent order\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(\n",
    "            block_in, in_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z to block_in\n",
    "        # middle\n",
    "        h = self.mid.block_2(self.mid.attn_1(self.mid.block_1(self.conv_in(z))))\n",
    "\n",
    "        # upsampling\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                h = self.up[i_level].block[i_block](h)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # end\n",
    "        h = self.conv_out(F.silu(self.norm_out(h), inplace=True))\n",
    "        return h\n",
    "\n",
    "\n",
    "class SonnetExponentialMovingAverage(nn.Module):\n",
    "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
    "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
    "    # of \"Neural Discrete Representation Learning\".\n",
    "    def __init__(self, decay, shape):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        self.counter = 0\n",
    "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
    "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
    "\n",
    "    def update(self, value):\n",
    "        self.counter += 1\n",
    "        with torch.no_grad():\n",
    "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
    "            self.average = self.hidden / (1 - self.decay**self.counter)\n",
    "\n",
    "    def __call__(self, value):\n",
    "        self.update(value)\n",
    "        return self.average\n",
    "\n",
    "\n",
    "class Codebook(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
    "        super().__init__()\n",
    "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
    "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.use_ema = use_ema\n",
    "        # Weight for the exponential moving average.\n",
    "        self.decay = decay\n",
    "        # Small constant to avoid numerical instability in embedding updates.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Dictionary embeddings.\n",
    "        limit = 3**0.5\n",
    "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_(\n",
    "            -limit, limit\n",
    "        )\n",
    "        if use_ema:\n",
    "            self.register_buffer(\"e_i_ts\", e_i_ts)\n",
    "        else:\n",
    "            self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
    "\n",
    "        # Exponential moving average of the cluster counts.\n",
    "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
    "        # Exponential moving average of the embeddings.\n",
    "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
    "        distances = (\n",
    "            (flat_x**2).sum(1, keepdim=True)\n",
    "            - 2 * flat_x @ self.e_i_ts\n",
    "            + (self.e_i_ts**2).sum(0, keepdim=True)\n",
    "        )\n",
    "        encoding_indices = distances.argmin(1)\n",
    "        quantized_x = F.embedding(\n",
    "            encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)\n",
    "        ).permute(0, 3, 1, 2)\n",
    "\n",
    "        # See second term of Equation (3).\n",
    "        if not self.use_ema:\n",
    "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
    "        else:\n",
    "            dictionary_loss = None\n",
    "\n",
    "        # See third term of Equation (3).\n",
    "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
    "        # Straight-through gradient. See Section 3.2.\n",
    "        quantized_x = x + (quantized_x - x).detach()\n",
    "\n",
    "        if self.use_ema and self.training:\n",
    "            with torch.no_grad():\n",
    "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
    "\n",
    "                # Cluster counts.\n",
    "                encoding_one_hots = F.one_hot(\n",
    "                    encoding_indices, self.num_embeddings\n",
    "                ).type(flat_x.dtype)\n",
    "                n_i_ts = encoding_one_hots.sum(0)\n",
    "                # Updated exponential moving average of the cluster counts.\n",
    "                # See Equation (6).\n",
    "                self.N_i_ts(n_i_ts)\n",
    "\n",
    "                # Exponential moving average of the embeddings. See Equation (7).\n",
    "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
    "                self.m_i_ts(embed_sums)\n",
    "\n",
    "                # This is kind of weird.\n",
    "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
    "                # and Equation (8).\n",
    "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
    "                N_i_ts_stable = (\n",
    "                    (self.N_i_ts.average + self.epsilon)\n",
    "                    / (N_i_ts_sum + self.num_embeddings * self.epsilon)\n",
    "                    * N_i_ts_sum\n",
    "                )\n",
    "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
    "\n",
    "        return (\n",
    "            quantized_x,\n",
    "            dictionary_loss,\n",
    "            commitment_loss,\n",
    "            encoding_indices.view(x.shape[0], -1),\n",
    "        )\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_hiddens,\n",
    "        ch_mult,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "        embedding_dim,\n",
    "        num_embeddings,\n",
    "        use_ema,\n",
    "        decay,\n",
    "        epsilon,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            ch=num_hiddens,\n",
    "            ch_mult=ch_mult,\n",
    "            num_res_blocks=num_residual_layers,\n",
    "            dropout=0.0,\n",
    "            in_channels=in_channels,\n",
    "            z_channels=embedding_dim,\n",
    "            double_z=False,\n",
    "            using_sa=True,\n",
    "            using_mid_sa=True,\n",
    "        )\n",
    "        self.pre_vq_conv = nn.Conv2d(\n",
    "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
    "        )\n",
    "        self.vq = Codebook(embedding_dim, num_embeddings, use_ema, decay, epsilon)\n",
    "        self.decoder = Decoder(\n",
    "            ch=num_hiddens,\n",
    "            ch_mult=(1, 2, 4, 8),\n",
    "            num_res_blocks=num_residual_layers,\n",
    "            dropout=0.0,\n",
    "            in_channels=out_channels,\n",
    "            z_channels=embedding_dim,\n",
    "            using_sa=True,\n",
    "            using_mid_sa=True,\n",
    "        )\n",
    "\n",
    "    def quantize(self, x):\n",
    "        z = self.pre_vq_conv(self.encoder(x))\n",
    "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
    "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = (\n",
    "            self.quantize(x)\n",
    "        )\n",
    "        x_recon = self.decoder(z_quantized)\n",
    "        return {\n",
    "            \"dictionary_loss\": dictionary_loss,\n",
    "            \"commitment_loss\": commitment_loss,\n",
    "            \"x_recon\": x_recon,\n",
    "            \"encoding_indices\": encoding_indices,\n",
    "        }\n",
    "\n",
    "\n",
    "class VQVAE_Trainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size=128,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        num_hiddens=64,\n",
    "        num_downsampling_layers=4,\n",
    "        num_residual_layers=4,\n",
    "        num_residual_hiddens=128,\n",
    "        embedding_dim=64,  # 32, 64, 128, 256\n",
    "        num_embeddings=512,  # 256, 512, 1024, 2048\n",
    "        use_ema=True,\n",
    "        decay=0.99,\n",
    "        epsilon=1e-5,\n",
    "        beta=0.25,\n",
    "        lr=2e-4,\n",
    "        weight_decay=0.0,\n",
    "        fid_features=2048,\n",
    "        batch_size=64,  # 128\n",
    "        dataset=\"celeba_hq\",\n",
    "    ):\n",
    "        super(VQVAE_Trainer, self).__init__()\n",
    "        self.model = VQVAE(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_downsampling_layers=num_downsampling_layers,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            num_residual_hiddens=num_residual_hiddens,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_embeddings=num_embeddings,\n",
    "            use_ema=use_ema,\n",
    "            decay=decay,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "\n",
    "        self.beta = beta\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x)\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        if out[\"dictionary_loss\"] is not None:\n",
    "            loss += out[\"dictionary_loss\"]\n",
    "            self.log(\"train_dictionary_loss\", out[\"dictionary_loss\"])\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_recon_error\", recon_error)\n",
    "        self.log(\"train_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x)\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        if out[\"dictionary_loss\"] is not None:\n",
    "            loss += out[\"dictionary_loss\"]\n",
    "            self.log(\"val_dictionary_loss\", out[\"dictionary_loss\"])\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_recon_error\", recon_error)\n",
    "        self.log(\"val_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            if self.global_step == 0 and batch_idx == 0:\n",
    "                self.logger.experiment.log(\n",
    "                    {\n",
    "                        \"original\": wandb.Image(\n",
    "                            torchvision.utils.make_grid(x[:64], nrow=8),\n",
    "                            caption=\"Real Image\",\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            self.logger.experiment.log(\n",
    "                {\n",
    "                    \"reconstructed\": wandb.Image(\n",
    "                        torchvision.utils.make_grid(out[\"x_recon\"][:64], nrow=8),\n",
    "                        caption=f\"Step {self.global_step}\",\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def on_test_start(self):\n",
    "        self.fid = FrechetInceptionDistance(self.hparams.fid_features).cpu()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        # Resize to 299x299\n",
    "        x = F.interpolate(x, size=299)\n",
    "        x_hat = F.interpolate(out[\"x_recon\"], size=299)\n",
    "\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "            x_hat = x_hat.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Convert to uint8\n",
    "        x = (x * 255).to(torch.uint8).cpu()\n",
    "        x_hat = (x_hat * 255).to(torch.uint8).cpu()\n",
    "\n",
    "        # Compute FID\n",
    "        self.fid.update(x, real=True)\n",
    "        self.fid.update(x_hat, real=False)\n",
    "\n",
    "        fid_score = self.fid.compute()\n",
    "        self.log(\"fid_score\", fid_score)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            amsgrad=True,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    (\n",
    "                        torchvision.transforms.Grayscale()\n",
    "                        if self.hparams.in_channels == 1\n",
    "                        else torchvision.transforms.Lambda(lambda x: x)\n",
    "                    ),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=True, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=True, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/train\", transform=transform\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=False, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=False, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\", transform=transform\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=False, transform=transform, download=True\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 16))\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=False, transform=transform, download=True\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 16))\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\", transform=transform\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 4))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # torch.set_float32_matmul_precision(\"high\")\n",
    "    vae = VQVAE_Trainer()\n",
    "    wandb_logger = WandbLogger(\n",
    "        name=\"vqvae\",\n",
    "        save_dir=\"./vqvae/logs\",\n",
    "        project=\"vq-vae\",\n",
    "        save_code=True,\n",
    "        log_model=True,\n",
    "    )\n",
    "    wandb_logger.watch(vae.model, log=\"all\", log_freq=100, log_graph=True)\n",
    "    trainer = pl.Trainer(\n",
    "        logger=wandb_logger,\n",
    "        check_val_every_n_epoch=1,\n",
    "        default_root_dir=\"./vqvae/logs\",\n",
    "        max_steps=100_000,  # 250_000\n",
    "    )\n",
    "    trainer.fit(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using 646,464 parameters\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(\n",
    "    ch=32,\n",
    "    ch_mult=(1, 1, 2, 2),\n",
    "    num_res_blocks=2,\n",
    "    dropout=0.0,\n",
    "    in_channels=3,\n",
    "    z_channels=64,\n",
    "    double_z=False,\n",
    "    using_sa=True,\n",
    "    using_mid_sa=True,\n",
    ")\n",
    "\n",
    "print(f\"Model is using {sum(p.numel() for p in encoder.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using 904,035 parameters\n"
     ]
    }
   ],
   "source": [
    "decoer = Decoder(\n",
    "    ch=32,\n",
    "    ch_mult=(1, 1, 2, 2),\n",
    "    num_res_blocks=2,\n",
    "    dropout=0.0,\n",
    "    in_channels=3,\n",
    "    z_channels=64,\n",
    "    using_sa=True,\n",
    "    using_mid_sa=True,\n",
    ")\n",
    "\n",
    "print(f\"Model is using {sum(p.numel() for p in decoer.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = Codebook(64, 512, True, 0.99, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
