{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    One of the other reviewers has mentioned that ...\n",
      "1    A wonderful little production. \\n\\nThe filming...\n",
      "2    I thought this was a wonderful way to spend ti...\n",
      "3    Basically there's a family where a little boy ...\n",
      "4    Petter Mattei's \"Love in the Time of Money\" is...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('projects/blog/4-reviews/IMDB Dataset.csv')['review'].apply(lambda x: x.replace('<br />', '\\n'))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"projects/blog/4-all-shakespeare/tokenizer.json\")\n",
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 5,378,662 characters\n"
     ]
    }
   ],
   "source": [
    "with open(\"projects/blog/4-all-shakespeare/shakespeare.txt\", \"r\") as f:\n",
    "    corpus = f.read()\n",
    "print(f\"Corpus has {len(corpus):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded corpus has 2,774,526 tokens\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(corpus).ids\n",
    "print(f\"Encoded corpus has {len(encoded):,} tokens\")\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 2,219,620 tokens\n",
      "Validation data has 554,906 tokens\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "train_size = int(len(encoded) * train_split)\n",
    "train_data = encoded[:train_size]\n",
    "val_data = encoded[train_size:]\n",
    "\n",
    "print(f\"Training data has {len(train_data):,} tokens\")\n",
    "print(f\"Validation data has {len(val_data):,} tokens\")\n",
    "del encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset and round to nearest 256\n",
    "seq_len = 256\n",
    "train_dataset = torch.tensor(train_data[:-(len(train_data) % seq_len)]).view(-1, seq_len)\n",
    "val_dataset = torch.tensor(val_data[:-(len(val_data) % seq_len)]).view(-1, seq_len)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # RNN\n",
    "        self.model = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to predict each character\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device)\n",
    "        \n",
    "        # GRU output along with new hidden state\n",
    "        out, hidden = self.model(x, hidden)\n",
    "        \n",
    "        # Reshape output for the fully connected layer\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def generate(self, ctx, hidden=None, max_len=256, temperature=1.0):\n",
    "        assert 0 <= temperature <= 1, \"Temperature has to be between 0 and 1\"\n",
    "\n",
    "        # Set the model to evaluation\n",
    "        self.eval()\n",
    "\n",
    "        # Convert the context to a tensor\n",
    "        ctx = torch.tensor(ctx, device=device).view(1, -1)\n",
    "\n",
    "        result = [ctx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_len-len(ctx[0])):\n",
    "                # Get the output and hidden state\n",
    "                output, hidden = self(ctx, hidden)\n",
    "\n",
    "                # Apply temperature\n",
    "                output = output.view(-1).div(temperature).exp()\n",
    "\n",
    "                # Sample the next character\n",
    "                char = torch.multinomial(output, 1).item()\n",
    "\n",
    "                # Append to the result\n",
    "                result.append(torch.tensor(char, device=device).view(1, 1))\n",
    "\n",
    "                # Update the context\n",
    "                ctx = torch.cat(result[-1:], dim=-1)\n",
    "\n",
    "        return torch.cat(result, dim=-1).view(-1).tolist()\n",
    "\n",
    "# Create an instance of the updated model\n",
    "vocab_size = 512\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 6\n",
    "\n",
    "model = Model(vocab_size, embed_dim, hidden_dim, num_layers).to(device)\n",
    "num_train_steps = 0\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # GRU\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to predict each character\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device)\n",
    "        \n",
    "        # GRU output along with new hidden state\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # Reshape output for the fully connected layer\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def generate(self, ctx, hidden=None, max_len=256, temperature=1.0):\n",
    "        assert 0 <= temperature <= 1, \"Temperature has to be between 0 and 1\"\n",
    "\n",
    "        # Set the model to evaluation\n",
    "        self.eval()\n",
    "\n",
    "        # Convert the context to a tensor\n",
    "        ctx = torch.tensor(ctx, device=device).view(1, -1)\n",
    "\n",
    "        result = [ctx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(max_len-len(ctx[0])):\n",
    "                # Get the output and hidden state\n",
    "                output, hidden = self(ctx, hidden)\n",
    "\n",
    "                # Apply temperature\n",
    "                output = output.view(-1).div(temperature).exp()\n",
    "\n",
    "                # Sample the next character\n",
    "                char = torch.multinomial(output, 1).item()\n",
    "\n",
    "                # Append to the result\n",
    "                result.append(torch.tensor(char, device=device).view(1, 1))\n",
    "\n",
    "                # Update the context\n",
    "                ctx = torch.cat(result[-1:], dim=-1)\n",
    "\n",
    "        return torch.cat(result, dim=-1).view(-1).tolist()\n",
    "\n",
    "# Create an instance of the updated model\n",
    "vocab_size = 512  # number of unique characters\n",
    "embed_dim = 128   # embedding dimension\n",
    "hidden_dim = 256  # LSTM hidden dimensions\n",
    "num_layers = 6  # number of GRU layers\n",
    "\n",
    "model = Model(vocab_size, embed_dim, hidden_dim, num_layers).to(device)\n",
    "num_train_steps = 0\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm.tqdm(loader, desc=\"Evaluation\"):\n",
    "        batch = batch.to(device)\n",
    "        x, y = batch[:, :-1], batch[:, 1:]\n",
    "        \n",
    "        output, _ = model(x)\n",
    "        \n",
    "        loss = criterion(output, y.flatten())\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "test_loss = evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_pytorch import EMA\n",
    "ema = EMA(model, beta=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "pbar = tqdm.tqdm(range(20), desc=\"Training\")\n",
    "for epoch in pbar:\n",
    "    test_loss = evaluate(model, train_loader)\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(train_loader, leave=True, desc=f\"Epoch {epoch}\")\n",
    "    for seq in pbar:\n",
    "        seq = seq.to(device)\n",
    "        x = seq[:,:-1]\n",
    "        y = seq[:,1:]\n",
    "            \n",
    "        # Forward pass\n",
    "        output, _ = model(x)\n",
    "        loss = criterion(output, y.flatten())\n",
    "            \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ema.update()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        pbar.set_postfix_str(f\"Loss: {loss.item():.4f}, Test Loss: {test_loss:.4f}, Step: {num_train_steps}\")\n",
    "        num_train_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new name\n",
    "context = tokenizer.encode(\"First Citizen\").ids\n",
    "generated = model.generate(context, max_len=256, temperature=1)\n",
    "print(print(tokenizer.decode(generated)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(model.state_dict(), 'projects/4-shakespeare/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
