{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2 = nn.Conv2d(4, 12, 5)\n",
    "        self.fc1 = nn.Linear(12*4*4, 10)\n",
    "\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'conv1': self.conv1,\n",
    "            'conv2': self.conv2,\n",
    "            'fc1': self.fc1\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        x = x.view(-1, 12*4*4)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = Model(1, 10)\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to onnx\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\", verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load an ONNX model\n",
    "model_path = 'model.onnx'\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Print a summary of the model\n",
    "print('IR version:', model.ir_version)\n",
    "print('Producer name:', model.producer_name)\n",
    "print('Graph name:', model.graph.name)\n",
    "print('Number of nodes:', len(model.graph.node))\n",
    "\n",
    "print()\n",
    "\n",
    "# List all nodes\n",
    "for node in model.graph.node:\n",
    "    print('Node:', node.name)\n",
    "    print('Inputs:', node.input)\n",
    "    print('Outputs:', node.output)\n",
    "    print('Type:', node.op_type)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import onnx\n",
    "\n",
    "\n",
    "def format_attributes(attributes: list[onnx.AttributeProto]) -> str:\n",
    "    \"\"\"Format node attributes for display.\"\"\"\n",
    "    attr_str = \"\"\n",
    "    for attr in attributes:\n",
    "        # Convert attribute to a readable format based on type\n",
    "        if attr.type == onnx.AttributeProto.INT:\n",
    "            attr_value = attr.i\n",
    "        elif attr.type == onnx.AttributeProto.FLOAT:\n",
    "            attr_value = attr.f\n",
    "        elif attr.type == onnx.AttributeProto.STRING:\n",
    "            attr_value = attr.s.decode()  # Assuming byte string\n",
    "        elif attr.type == onnx.AttributeProto.INTS:\n",
    "            attr_value = list(attr.ints)\n",
    "        elif attr.type == onnx.AttributeProto.FLOATS:\n",
    "            attr_value = list(attr.floats)\n",
    "        elif attr.type == onnx.AttributeProto.STRINGS:\n",
    "            attr_value = [x.decode() for x in attr.strings]\n",
    "        else:\n",
    "            attr_value = \"[Unsupported attribute type]\"\n",
    "        attr_str += f\"{attr.name}: {attr_value}\\n\"\n",
    "    return attr_str.strip()\n",
    "\n",
    "\n",
    "def visualize_model_flow_with_attributes(onnx_model: onnx.ModelProto) -> Digraph:\n",
    "    dot = Digraph(comment=\"Model Visualization with Attributes\", format=\"png\")\n",
    "\n",
    "    # Create a mapping from output tensors to node names\n",
    "    tensor_to_node = {}\n",
    "\n",
    "    # Add nodes for input and output tensors\n",
    "    for input_tensor in onnx_model.graph.input:\n",
    "        tensor_to_node[input_tensor.name] = input_tensor.name\n",
    "\n",
    "    # Create nodes for each operation\n",
    "    for node in onnx_model.graph.node:\n",
    "        for output in node.output:\n",
    "            tensor_to_node[output] = node.name\n",
    "\n",
    "    for output_tensor in onnx_model.graph.output:\n",
    "        tensor_to_node[output_tensor.name] = output_tensor.name\n",
    "\n",
    "    print(tensor_to_node)\n",
    "\n",
    "    # Add nodes and edges\n",
    "    for node in onnx_model.graph.node:\n",
    "        attributes = format_attributes(node.attribute)\n",
    "        label = f\"{node.op_type}\\n{node.name}\\n{attributes}\"\n",
    "        dot.node(node.name, label, shape=\"box\")\n",
    "        for input_tensor in node.input:\n",
    "            if input_tensor in tensor_to_node:\n",
    "                # Connect the output of the previous node to the current node\n",
    "                dot.edge(tensor_to_node[input_tensor], node.name)\n",
    "\n",
    "    return dot\n",
    "\n",
    "\n",
    "# Load your model\n",
    "model_path = \"model.onnx\"\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Visualize the model flow with attributes\n",
    "dot = visualize_model_flow_with_attributes(model)\n",
    "# Save and render the visualization\n",
    "dot.render(\"output/model_flow_with_attributes_visualization\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles: 128 x 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/y4w577ts6qb9hpql_4lr_km40000gn/T/ipykernel_57803/1089791483.py:37: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tqdm.tnrange(num_tiles_x):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33deb06ee12e4603aa45a6daa1f3bc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "def load_images(image_dir, tile_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(tile_size),  # Resize to uniform size\n",
    "        transforms.ToTensor()           # Convert to tensor\n",
    "    ])\n",
    "    dataset = ImageFolder(root=image_dir, transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    all_images, _ = next(iter(loader))\n",
    "    return all_images.to(device)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    return transforms.ToPILImage()(tensor)\n",
    "\n",
    "def assemble_mosaic(target_image_path, small_images, tile_size=(64, 64), mosaic_size=(1024, 1024)):\n",
    "    target_image = Image.open(target_image_path)\n",
    "    target_image = target_image.resize(mosaic_size)\n",
    "    target_tensor = transforms.ToTensor()(target_image).to(device)\n",
    "\n",
    "    # Calculate number of tiles\n",
    "    num_tiles_x = mosaic_size[0] // tile_size[0]\n",
    "    num_tiles_y = mosaic_size[1] // tile_size[1]\n",
    "\n",
    "    print(f'Number of tiles: {num_tiles_x} x {num_tiles_y}')\n",
    "\n",
    "    # Initialize mosaic tensor\n",
    "    mosaic = torch.zeros(3, mosaic_size[1], mosaic_size[0], device=device)\n",
    "\n",
    "    for i in tqdm.tnrange(num_tiles_x):\n",
    "        for j in range(num_tiles_y):\n",
    "            x = i * tile_size[0]\n",
    "            y = j * tile_size[1]\n",
    "            region = target_tensor[:, y:y+tile_size[1], x:x+tile_size[0]]\n",
    "            avg_color = region.reshape(3, -1).mean(dim=1)\n",
    "\n",
    "            # Find the closest tile\n",
    "            distances = torch.norm(small_images - avg_color[:, None, None], dim=1, p=2).mean([1, 2])\n",
    "            closest_img_idx = torch.argmin(distances)\n",
    "            closest_img = small_images[closest_img_idx]\n",
    "\n",
    "            # Place tile into mosaic\n",
    "            mosaic[:, y:y+tile_size[1], x:x+tile_size[0]] = closest_img\n",
    "\n",
    "    return tensor_to_image(mosaic)\n",
    "\n",
    "# Usage\n",
    "tile_size = (32, 32)\n",
    "mosaic_size = (4096, 4096)\n",
    "image_dir = 'data/celeba_hq/val'\n",
    "target_image_path = 'data/celeba_hq/val/male/000080.jpg'\n",
    "\n",
    "small_images = load_images(image_dir, tile_size)\n",
    "mosaic = assemble_mosaic(target_image_path, small_images, tile_size, mosaic_size)\n",
    "mosaic.save('mosaic.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
