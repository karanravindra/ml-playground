{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import tqdm as tqdm\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        layers = []\n",
    "        for i in range(num_residual_layers):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=num_hiddens,\n",
    "                        out_channels=num_residual_hiddens,\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=num_residual_hiddens,\n",
    "                        out_channels=num_hiddens,\n",
    "                        kernel_size=1,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = h + layer(h)\n",
    "\n",
    "        # ResNet V1-style.\n",
    "        return torch.relu(h)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        num_hiddens,\n",
    "        num_downsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
    "        # off with a ReLU.\n",
    "        conv = nn.Sequential()\n",
    "        for downsampling_layer in range(num_downsampling_layers):\n",
    "            if downsampling_layer == 0:\n",
    "                out_channels = num_hiddens // 2\n",
    "            elif downsampling_layer == 1:\n",
    "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
    "\n",
    "            else:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
    "\n",
    "            conv.add_module(\n",
    "                f\"down{downsampling_layer}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            )\n",
    "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
    "\n",
    "        conv.add_module(\n",
    "            \"final_conv\",\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_hiddens,\n",
    "                out_channels=num_hiddens,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "        self.conv = conv\n",
    "        self.residual_stack = ResidualStack(\n",
    "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        return self.residual_stack(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_hiddens,\n",
    "        num_upsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "        out,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=num_hiddens,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.residual_stack = ResidualStack(\n",
    "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
    "        )\n",
    "        upconv = nn.Sequential()\n",
    "        for upsampling_layer in range(num_upsampling_layers):\n",
    "            if upsampling_layer < num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
    "\n",
    "            elif upsampling_layer == num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
    "\n",
    "            elif upsampling_layer > num_upsampling_layers - 2:\n",
    "                (in_channels, out_channels) = (num_hiddens // 2, out)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid upsampling layer: {upsampling_layer}. In Encoder.\"\n",
    "                )\n",
    "\n",
    "            upconv.add_module(\n",
    "                f\"up{upsampling_layer}\",\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            )\n",
    "            if upsampling_layer < num_upsampling_layers - 1:\n",
    "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
    "\n",
    "        self.upconv = upconv\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = self.residual_stack(h)\n",
    "        x_recon = self.upconv(h)\n",
    "        return F.sigmoid(x_recon)\n",
    "\n",
    "class SonnetExponentialMovingAverage(nn.Module):\n",
    "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
    "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
    "    # of \"Neural Discrete Representation Learning\".\n",
    "    def __init__(self, decay, shape):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        self.counter = 0\n",
    "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
    "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
    "\n",
    "    def update(self, value):\n",
    "        self.counter += 1\n",
    "        with torch.no_grad():\n",
    "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
    "            self.average = self.hidden / (1 - self.decay**self.counter)\n",
    "\n",
    "    def __call__(self, value):\n",
    "        self.update(value)\n",
    "        return self.average\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
    "        super().__init__()\n",
    "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
    "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.use_ema = use_ema\n",
    "        # Weight for the exponential moving average.\n",
    "        self.decay = decay\n",
    "        # Small constant to avoid numerical instability in embedding updates.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Dictionary embeddings.\n",
    "        limit = 3**0.5\n",
    "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_(\n",
    "            -limit, limit\n",
    "        )\n",
    "        if use_ema:\n",
    "            self.register_buffer(\"e_i_ts\", e_i_ts)\n",
    "        else:\n",
    "            self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
    "\n",
    "        # Exponential moving average of the cluster counts.\n",
    "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
    "        # Exponential moving average of the embeddings.\n",
    "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
    "        distances = (\n",
    "            (flat_x**2).sum(1, keepdim=True)\n",
    "            - 2 * flat_x @ self.e_i_ts\n",
    "            + (self.e_i_ts**2).sum(0, keepdim=True)\n",
    "        )\n",
    "        encoding_indices = distances.argmin(1)\n",
    "        quantized_x = F.embedding(\n",
    "            encoding_indices.view(x.shape[0], x.shape[2], x.shape[3]), self.e_i_ts.transpose(0, 1)\n",
    "        ).permute(0, 3, 1, 2)\n",
    "\n",
    "        # See second term of Equation (3).\n",
    "        if not self.use_ema:\n",
    "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
    "        else:\n",
    "            dictionary_loss = None\n",
    "\n",
    "        # See third term of Equation (3).\n",
    "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
    "        # Straight-through gradient. See Section 3.2.\n",
    "        quantized_x = x + (quantized_x - x).detach()\n",
    "\n",
    "        if self.use_ema and self.training:\n",
    "            with torch.no_grad():\n",
    "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
    "\n",
    "                # Cluster counts.\n",
    "                encoding_one_hots = F.one_hot(\n",
    "                    encoding_indices, self.num_embeddings\n",
    "                ).type(flat_x.dtype)\n",
    "                n_i_ts = encoding_one_hots.sum(0)\n",
    "                # Updated exponential moving average of the cluster counts.\n",
    "                # See Equation (6).\n",
    "                self.N_i_ts(n_i_ts)\n",
    "\n",
    "                # Exponential moving average of the embeddings. See Equation (7).\n",
    "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
    "                self.m_i_ts(embed_sums)\n",
    "\n",
    "                # This is kind of weird.\n",
    "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
    "                # and Equation (8).\n",
    "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
    "                N_i_ts_stable = (\n",
    "                    (self.N_i_ts.average + self.epsilon)\n",
    "                    / (N_i_ts_sum + self.num_embeddings * self.epsilon)\n",
    "                    * N_i_ts_sum\n",
    "                )\n",
    "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
    "\n",
    "        return (\n",
    "            quantized_x,\n",
    "            dictionary_loss,\n",
    "            commitment_loss,\n",
    "            encoding_indices.view(x.shape[0], -1),\n",
    "        )\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_hiddens,\n",
    "        num_downsampling_layers,\n",
    "        num_residual_layers,\n",
    "        num_residual_hiddens,\n",
    "        embedding_dim,\n",
    "        num_embeddings,\n",
    "        use_ema,\n",
    "        decay,\n",
    "        epsilon,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            in_channels,\n",
    "            num_hiddens,\n",
    "            num_downsampling_layers,\n",
    "            num_residual_layers,\n",
    "            num_residual_hiddens,\n",
    "        )\n",
    "        self.pre_vq_conv = nn.Conv2d(\n",
    "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
    "        )\n",
    "        self.vq = VectorQuantizer(\n",
    "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim,\n",
    "            num_hiddens,\n",
    "            num_downsampling_layers,\n",
    "            num_residual_layers,\n",
    "            num_residual_hiddens,\n",
    "            out_channels,\n",
    "        )\n",
    "\n",
    "    def quantize(self, x):\n",
    "        z = self.pre_vq_conv(self.encoder(x))\n",
    "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
    "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = (\n",
    "            self.quantize(x)\n",
    "        )\n",
    "        x_recon = self.decoder(z_quantized)\n",
    "        return {\n",
    "            \"dictionary_loss\": dictionary_loss,\n",
    "            \"commitment_loss\": commitment_loss,\n",
    "            \"x_recon\": x_recon,\n",
    "            \"encoding_indices\": encoding_indices,\n",
    "        }\n",
    "\n",
    "class VQVAE_Trainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size=128,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        num_hiddens=64,\n",
    "        num_downsampling_layers=2,\n",
    "        num_residual_layers=4,\n",
    "        num_residual_hiddens=128,\n",
    "        embedding_dim=32,  # 32, 64, 128, 256\n",
    "        num_embeddings=2048,  # 256, 512, 1024, 2048\n",
    "        use_ema=True,\n",
    "        decay=0.99,\n",
    "        epsilon=1e-5,\n",
    "        beta=0.25,\n",
    "        lr=4e-4,\n",
    "        weight_decay=0.01,\n",
    "        fid_features=2048,\n",
    "        batch_size=64,\n",
    "        dataset=\"celeba_hq\",\n",
    "    ):\n",
    "        super(VQVAE_Trainer, self).__init__()\n",
    "        self.model = VQVAE(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_downsampling_layers=num_downsampling_layers,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            num_residual_hiddens=num_residual_hiddens,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_embeddings=num_embeddings,\n",
    "            use_ema=use_ema,\n",
    "            decay=decay,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "\n",
    "        self.beta = beta\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x)\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        if out[\"dictionary_loss\"] is not None:\n",
    "            loss += out[\"dictionary_loss\"]\n",
    "            self.log(\"train_dictionary_loss\", out[\"dictionary_loss\"])\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_recon_error\", recon_error)\n",
    "        self.log(\"train_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        recon_error = F.mse_loss(out[\"x_recon\"], x)\n",
    "\n",
    "        loss = recon_error + self.beta * out[\"commitment_loss\"]\n",
    "\n",
    "        if out[\"dictionary_loss\"] is not None:\n",
    "            loss += out[\"dictionary_loss\"]\n",
    "            self.log(\"val_dictionary_loss\", out[\"dictionary_loss\"])\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_recon_error\", recon_error)\n",
    "        self.log(\"val_commitment_loss\", out[\"commitment_loss\"])\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            if self.global_step == 0 and batch_idx == 0:\n",
    "                self.logger.experiment.log(\n",
    "                    {\n",
    "                        \"original\": wandb.Image(\n",
    "                            torchvision.utils.make_grid(x[:64], nrow=8),\n",
    "                            caption=\"Real Image\",\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            self.logger.experiment.log(\n",
    "                {\n",
    "                    \"reconstructed\": wandb.Image(\n",
    "                        torchvision.utils.make_grid(out[\"x_recon\"][:64], nrow=8),\n",
    "                        caption=f\"Step {self.global_step}\",\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def on_test_start(self):\n",
    "        self.fid = FrechetInceptionDistance(self.hparams.fid_features).cpu()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        # Resize to 299x299\n",
    "        x = F.interpolate(x, size=299)\n",
    "        x_hat = F.interpolate(out[\"x_recon\"], size=299)\n",
    "\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "            x_hat = x_hat.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Convert to uint8\n",
    "        x = (x * 255).to(torch.uint8).cpu()\n",
    "        x_hat = (x_hat * 255).to(torch.uint8).cpu()\n",
    "\n",
    "        # Compute FID\n",
    "        self.fid.update(x, real=True)\n",
    "        self.fid.update(x_hat, real=False)\n",
    "\n",
    "        fid_score = self.fid.compute()\n",
    "        self.log(\"fid_score\", fid_score)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            amsgrad=True,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    (\n",
    "                        torchvision.transforms.Grayscale()\n",
    "                        if self.hparams.in_channels == 1\n",
    "                        else torchvision.transforms.Lambda(lambda x: x)\n",
    "                    ),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=True, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=True, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/train\", transform=transform\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=False, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=False, transform=transform, download=True\n",
    "            )\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\", transform=transform\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.hparams.dataset == \"mnist\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.MNIST(\n",
    "                root=\"data/mnist\", train=False, transform=transform, download=True\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 16))\n",
    "\n",
    "        elif self.hparams.dataset == \"cifar10\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.CIFAR10(\n",
    "                root=\"data/cifar10\", train=False, transform=transform, download=True\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 16))\n",
    "\n",
    "        elif self.hparams.dataset == \"celeba_hq\":\n",
    "            transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize(self.hparams.sample_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "            dataset = torchvision.datasets.ImageFolder(\n",
    "                \"data/celeba_hq/val\", transform=transform\n",
    "            )\n",
    "            # Return first 1/4\n",
    "            dataset = torch.utils.data.Subset(dataset, range(len(dataset) // 4))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.hparams.dataset}\")\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook shape: torch.Size([4, 16])\n",
      " torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "vqvae = VQVAE_Trainer.load_from_checkpoint(\"blog/10-vqvae/vqvae/mnist-32/model.ckpt\")\n",
    "vqvae = vqvae.model.to(device)\n",
    "\n",
    "print(\"Codebook shape:\", vqvae.vq.e_i_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.load(\"blog/10-vqvae/datasets/idx/mnist/train.pt\"),\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.load(\"blog/10-vqvae/datasets/idx/mnist/val.pt\"),\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 136,872 parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 32\n",
    "    block_size: int = 65\n",
    "    emb_size: int = 24\n",
    "    heads: int = 12\n",
    "    num_layers: int = 4\n",
    "    attn_dropout: float = 0.1\n",
    "    ff_mult: int = 4\n",
    "    ff_dropout: float = 0.1\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx, head_idx, cache_enabled=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_idx = head_idx\n",
    "        self.cache_enabled = cache_enabled\n",
    "        self.cache = {}\n",
    "\n",
    "        self.q = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.k = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.v = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.out = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "        cache_key = (B, T, C)  # Example key; adjust based on your caching strategy\n",
    "\n",
    "        if self.cache_enabled and cache_key in self.cache:\n",
    "            k, v = self.cache[cache_key][\"k\"], self.cache[cache_key][\"v\"]\n",
    "        else:\n",
    "            k = (\n",
    "                self.k(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            v = (\n",
    "                self.v(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            if self.cache_enabled:\n",
    "                self.cache[cache_key] = {\"k\": k, \"v\": v}\n",
    "\n",
    "        q = (\n",
    "            self.q(x)\n",
    "            .view(B, T, self.config.heads, C // self.config.heads)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / ((C // self.config.heads) ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out(x), attn\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(config, layer_idx=layer_idx, head_idx=i)\n",
    "                for i in range(config.heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # input and output are the same size\n",
    "        attns = []\n",
    "        for head in self.heads:\n",
    "            attn, _ = head(x, mask=mask)\n",
    "            attns.append(attn)\n",
    "\n",
    "        return torch.mean(torch.stack(attns), dim=0)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.emb_size)\n",
    "        self.attn = MaskedMultiHeadAttention(config, layer_idx)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(config.emb_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.emb_size, config.ff_mult * config.emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_mult * config.emb_size, config.emb_size),\n",
    "        )\n",
    "\n",
    "        if config.ff_dropout > 0:\n",
    "            self.ff_dropout = nn.Dropout(config.ff_dropout)\n",
    "\n",
    "        if config.attn_dropout > 0:\n",
    "            self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x, mask=mask)\n",
    "\n",
    "        if hasattr(self, \"attn_dropout\"):\n",
    "            x = self.attn_dropout(x)\n",
    "\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "\n",
    "        if hasattr(self, \"ff_dropout\"):\n",
    "            x = self.ff_dropout(x)\n",
    "\n",
    "        return x + identity\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.emb_size)\n",
    "        self.pe = nn.Parameter(torch.randn(config.block_size, config.emb_size))\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(config, layer_idx=i) for i in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm(config.emb_size)\n",
    "        self.head = nn.Linear(config.emb_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie weights\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        assert (\n",
    "            not T > self.config.block_size\n",
    "        ), \"Sequence length is longer than block size\"\n",
    "\n",
    "        emb = self.token_emb(x)\n",
    "        pe = self.pe[:T].unsqueeze(0)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), 1).bool()\n",
    "\n",
    "        x = emb + pe\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        # Input is a contiguous tensor\n",
    "        y = y.flatten()\n",
    "        y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "\n",
    "        return F.cross_entropy(y_pred, y)\n",
    "\n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        class_label: int | None = None,\n",
    "        max_len: int = 64,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 16,\n",
    "    ):\n",
    "        self.eval()\n",
    "\n",
    "        if class_label is not None:\n",
    "            generated = []\n",
    "            primer_t = torch.as_tensor(class_label, device=device).view(1, -1)\n",
    "\n",
    "        else:\n",
    "            classes = torch.arange(10, device=device).view(-1, 1)\n",
    "            generated = []\n",
    "            primer_t = classes\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            if primer_t.size(1) >= self.config.block_size:\n",
    "                primer_t = primer_t[:, -self.config.block_size :]\n",
    "\n",
    "            out = self(primer_t)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            out = F.softmax(out, dim=-1)\n",
    "            out = torch.topk(out, top_k, dim=-1)[0]\n",
    "            out = torch.multinomial(out, num_samples=1)\n",
    "\n",
    "            gen = out\n",
    "\n",
    "            generated.append(gen)\n",
    "\n",
    "            primer_t = torch.cat((primer_t, out), dim=1)\n",
    "\n",
    "        return torch.cat(generated, dim=1)\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {model.get_param_count():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "@torch.no_grad()\n",
    "def decode_idxs(idxs):\n",
    "    idxs = idxs.to(device)\n",
    "    quantized_x = F.embedding(\n",
    "        idxs.view(idxs.shape[0], 8, 8), vqvae.vq.e_i_ts.transpose(0, 1)\n",
    "    ).permute(0, 3, 1, 2)\n",
    "\n",
    "    # reconstruct\n",
    "    x_recon = vqvae.decoder(quantized_x)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(torchvision.utils.make_grid(x_recon[:64], nrow=10).cpu().numpy().transpose(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm.tqdm(val_loader):\n",
    "        y = y.to(\"mps\")\n",
    "        x = x.to(\"mps\") + 10\n",
    "\n",
    "        x = torch.cat([y.view(-1, 1), x], dim=1)\n",
    "\n",
    "        inputs = x[:, :-1]\n",
    "        targets = x[:, 1:]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = model.loss(targets, outputs)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_pytorch import EMA\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "ema = EMA(model, beta=0.999, update_after_step=100, update_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 34/79 [00:11<00:14,  3.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 30\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     27\u001b[0m inputs \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     28\u001b[0m targets \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(targets, outputs)\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 182\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m x \u001b[38;5;241m=\u001b[39m emb \u001b[38;5;241m+\u001b[39m pe\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 182\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 117\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    115\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[0;32m--> 117\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_dropout\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    120\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dropout(x)\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 85\u001b[0m, in \u001b[0;36mMaskedMultiHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     83\u001b[0m attns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads:\n\u001b[0;32m---> 85\u001b[0m     attn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     attns\u001b[38;5;241m.\u001b[39mappend(attn)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(attns), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/repos/ml-zoo/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 64\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m attn \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dropout(attn)\n\u001b[0;32m---> 64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, C)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x), attn\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(20):\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(train_loader, leave=False, desc=\"Epoch: %d\" % (epoch + 1))\n",
    "    for x, y in pbar:\n",
    "        y = y.to(device)\n",
    "        x = x.to(device) + 10\n",
    "\n",
    "        x = torch.cat([y.view(-1, 1), x], dim=1)\n",
    "\n",
    "        inputs = x[:, :-1]\n",
    "        targets = x[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = model.loss(targets, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ema.update()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), lr=str(scheduler._last_lr[0]).format(\"2e\"), val_loss=val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAABlCAYAAADK3JXbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACaUlEQVR4nO3dQQqDMABFQVO8/5XTRS+Q0GelMrN2ET5uHqKOOec8AAAAQq+7DwAAADyP0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgd65eOMa48hwAAMCfWPnntycaAABATmgAAAA5oQEAAOSEBgAAkPsqNLwgvs9m+2y2z2b7bLbPZvtsxq+41/bZrDfmyivjh/EBAIAPX50CAABuITQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHLn6oVzzivPAQAAPIgnGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQO4NxYcR3ds64MIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "decode_idxs(model.generate(temperature=0, top_k=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize attention head\n",
    "attn_head = model.blocks[0].attn.heads[0]\n",
    "\n",
    "x, y = next(iter(val_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "x = torch.cat([y.view(-1, 1), x], dim=1)\n",
    "\n",
    "inputs = x[:, :-1]\n",
    "\n",
    "inputs = model.token_emb(inputs)\n",
    "inputs = inputs + model.pe[:inputs.size(1)]\n",
    "inputs = model.blocks[0].ln1(inputs)\n",
    "\n",
    "attn = attn_head(inputs, mask=None)[1].detach()\n",
    "plt.imshow(attn[0, 0].cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(attn[0, 3].cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
