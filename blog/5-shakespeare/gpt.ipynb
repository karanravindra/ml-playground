{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from tokenizers import Tokenizer, models, decoders, trainers, tools, pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[MASK]\", \"[UNK]\"], vocab_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train([\"blog/5-shakespeare/data/train.txt\"], trainer=trainer)\n",
    "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"blog/5-shakespeare/data/train.txt\", \"r\") as f:\n",
    "    train_corpus = f.read()\n",
    "\n",
    "with open(\"blog/5-shakespeare/data/test.txt\", \"r\") as f:\n",
    "    test_corpus = f.read()\n",
    "\n",
    "train_encoded_corpus = tokenizer.encode(train_corpus).ids\n",
    "val_encoded_corpus = tokenizer.encode(test_corpus).ids\n",
    "\n",
    "del train_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, seq_len):\n",
    "        self.corpus = corpus\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.as_tensor(self.corpus[idx:idx+self.seq_len]), torch.as_tensor(self.corpus[idx+1:idx+self.seq_len+1])\n",
    "    \n",
    "\n",
    "seq_len = 64\n",
    "train_dataset = Dataset(train_encoded_corpus, seq_len)\n",
    "val_dataset = Dataset(val_encoded_corpus, seq_len)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "del train_encoded_corpus, val_encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model already exists, delete it\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "if \"optimizer\" in locals():\n",
    "    del optimizer\n",
    "if \"scheduler\" in locals():\n",
    "    del scheduler\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = tokenizer.get_vocab_size()\n",
    "    block_size: int = seq_len\n",
    "    emb_size: int = 128\n",
    "    heads: int = 8\n",
    "    num_layers: int = 8\n",
    "    attn_dropout: float = 0\n",
    "    ff_mult: int = 2\n",
    "    ff_dropout: float = 0\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.q = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.k = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.v = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.out = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q = (\n",
    "            self.q(x)\n",
    "            .view(B, T, self.config.heads, C // self.config.heads)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.k(x)\n",
    "            .view(B, T, self.config.heads, C // self.config.heads)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.v(x)\n",
    "            .view(B, T, self.config.heads, C // self.config.heads)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / ((C // self.config.heads) ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out(x), attn\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.heads)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # input and output are the same size\n",
    "        attns = []\n",
    "        for head in self.heads:\n",
    "            attn, _ = head(x, mask=mask)\n",
    "            attns.append(attn)\n",
    "\n",
    "        return torch.mean(torch.stack(attns), dim=0)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.emb_size)\n",
    "        self.attn = MaskedMultiHeadAttention(config)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(config.emb_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.emb_size, config.ff_mult * config.emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_mult * config.emb_size, config.emb_size),\n",
    "        )\n",
    "\n",
    "        if config.ff_dropout > 0:\n",
    "            self.ff_dropout = nn.Dropout(config.ff_dropout)\n",
    "\n",
    "        if config.attn_dropout > 0:\n",
    "            self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x, mask=mask)\n",
    "\n",
    "        if hasattr(self, \"attn_dropout\"):\n",
    "            x = self.attn_dropout(x)\n",
    "\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "\n",
    "        if hasattr(self, \"ff_dropout\"):\n",
    "            x = self.ff_dropout(x)\n",
    "\n",
    "        return x + identity\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.emb_size)\n",
    "        self.pos_emb = nn.Embedding(config.block_size, config.emb_size)\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(config.emb_size)\n",
    "        self.head = nn.Linear(config.emb_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie weights\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        assert T <= self.config.block_size, \"Sequence length is longer than block size\"\n",
    "\n",
    "        emb = self.token_emb(x)\n",
    "        pe = self.pos_emb(torch.arange(T-1, -1, step=-1, device=device))\n",
    "\n",
    "        x = emb + pe\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=torch.tril(torch.ones(T, T, device=device)).view(1, T, T))\n",
    "\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        # Input is a contiguous tensor\n",
    "        y = y.flatten()\n",
    "        y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "\n",
    "        return F.cross_entropy(y_pred, y)\n",
    "\n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, primer: str, max_len: int = 128, temperature: float = 1.0):\n",
    "        self.eval()\n",
    "\n",
    "        generated = tokenizer.encode(primer).ids\n",
    "        primer_t = torch.as_tensor(generated, device=device).unsqueeze(0)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            if primer_t.size(1) > self.config.block_size:\n",
    "                primer_t = primer_t[:, -self.config.block_size :]\n",
    "            out = self(primer_t)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            out = torch.multinomial(F.softmax(out, dim=-1), num_samples=1)\n",
    "\n",
    "            generated.append(out.item())\n",
    "\n",
    "            primer_t = torch.cat((primer_t, out), dim=1)\n",
    "\n",
    "        return tokenizer.decode(generated)\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {model.get_param_count():,} parameters\")\n",
    "# print(model.generate(\"First Citizen:\", max_len=64))\n",
    "\n",
    "del config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm.tqdm(dataloader, desc=\"Evaluation\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y, y_pred).item()\n",
    "        total_loss += loss\n",
    "        pbar.set_postfix({\"loss\": loss})\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "roll_loss = 4\n",
    "\n",
    "for epoch in range(100):\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=True)\n",
    "    val_loss =  evaluate(model, val_loader)\n",
    "\n",
    "    model.train()\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y, y_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_train_steps += 1\n",
    "        roll_loss = 0.9 * roll_loss + 0.1 * loss.item()\n",
    "\n",
    "        pbar.set_postfix_str(f\"loss: {roll_loss:.4f}, val_loss: {val_loss:.4f}, steps: {num_train_steps:,}\")\n",
    "\n",
    "        # assert num_train_steps != 100, \"Stop training\"\n",
    "\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.mean().item():4f}, {param.std().item():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(model.generate(\n",
    "    \"The Project Gutenberg eBook\",\n",
    "    max_len=64,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot postion embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos_emb = model.pos_emb.weight.detach().cpu().numpy()\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(pos_emb, aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Position Embeddings\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(model.pos_emb.weight.detach().cpu().numpy()[:, 0])\n",
    "plt.title(\"Position Embedding 0\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
